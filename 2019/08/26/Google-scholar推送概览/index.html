<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="Zhen Cheng">



<meta name="description" content="这里记录一些谷歌学术推送文章的概览，简述一些读到的idea，如果有值得深入阅读的，将会再进行深入阅读并写成完整笔记。由于很多文章是粗略地读的，很可能会有理解上的偏差，还望读者不吝指出。">
<meta name="keywords" content="Google scholar">
<meta property="og:type" content="article">
<meta property="og:title" content="Google scholar推送概览">
<meta property="og:url" content="http://Joechann0831.github.io/2019/08/26/Google-scholar推送概览/index.html">
<meta property="og:site_name" content="Zhen&#39;s blog">
<meta property="og:description" content="这里记录一些谷歌学术推送文章的概览，简述一些读到的idea，如果有值得深入阅读的，将会再进行深入阅读并写成完整笔记。由于很多文章是粗略地读的，很可能会有理解上的偏差，还望读者不吝指出。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190826-F-number1.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190826-F-number2.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190826-F-number3.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190827-OmniMVS1.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190827-OmniMVS2.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190827-ESRGAN1.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190827-ESRGAN2.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190828-RankSRGAN.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190828-Multiview.png">
<meta property="og:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190910-ReID.png">
<meta property="og:updated_time" content="2019-09-11T03:50:39.251Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Google scholar推送概览">
<meta name="twitter:description" content="这里记录一些谷歌学术推送文章的概览，简述一些读到的idea，如果有值得深入阅读的，将会再进行深入阅读并写成完整笔记。由于很多文章是粗略地读的，很可能会有理解上的偏差，还望读者不吝指出。">
<meta name="twitter:image" content="http://joechann0831.github.io/2019/08/26/Google-scholar推送概览/190826-F-number1.png">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Zhen&#39;s blog" type="application/atom+xml">



    <link rel="shortcut icon" href="/img/avatar.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Google scholar推送概览 | Zhen&#39;s blog</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script><!-- hexo-inject:begin --><link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css'><!-- hexo-inject:end -->






</head></html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Zhen Cheng</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Stay hungry, stay foolish.</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload>
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto: mywander@mail.ustc.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" href="/github.com/Joechann0831" title="GitHub"></a>
                            
                                <a class="fa 微信" href="/ergou-haoren" title="微信"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-reconstruction/">3D reconstruction</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Google-scholar/">Google scholar</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tech/">Tech</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-distillation/">data distillation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/denoising/">denoising</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/depth-estimation/">depth estimation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dynamic-filter-network/">dynamic filter network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/intrinsic-decomposition/">intrinsic decomposition</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kernel-prediction-networks/">kernel prediction networks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/light-field/">light field</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/network-design/">network design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/optical-flow/">optical flow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/real-world-SISR/">real-world SISR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-supervision/">self-supervision</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/super-resolution/">super resolution</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/video/">video</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/video-frame-interpolation/">video frame interpolation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/video-processing/">video processing</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://blog.ddlee.cn/">DDLee</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://zeyuxiao1997.github.io/">Zeyu Xiao</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">Focus on computational imaging</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Zhen Cheng</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Zhen Cheng</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Stay hungry, stay foolish.</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto: mywander@mail.ustc.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="/github.com/Joechann0831" title="GitHub"></a>
                            
                                <a class="fa 微信" target="_blank" href="/ergou-haoren" title="微信"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我">
</nav>
      <div class="body-wrap"><article id="post-Google-scholar推送概览" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/08/26/Google-scholar推送概览/" class="article-date">
      <time datetime="2019-08-26T07:58:34.000Z" itemprop="datePublished">2019-08-26</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Google scholar推送概览
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/Notes/">Notes</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Google-scholar/">Google scholar</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>这里记录一些谷歌学术推送文章的概览，简述一些读到的idea，如果有值得深入阅读的，将会再进行深入阅读并写成完整笔记。由于很多文章是粗略地读的，很可能会有理解上的偏差，还望读者不吝指出。</p>
<a id="more"></a>
<h2 id="20190826"><a class="markdownIt-Anchor" href="#20190826"></a> 2019.08.26</h2>
<h3 id="two-stream-sparse-network-for-accurate-image-super-resolution-icmew19"><a class="markdownIt-Anchor" href="#two-stream-sparse-network-for-accurate-image-super-resolution-icmew19"></a> Two-stream sparse network for accurate image super-resolution-ICMEW19</h3>
<p>这篇文章参考了ECCV18的Sparsely aggregated convolutional networks，完全dense连接的网络计算复杂度太高，而且容易过拟合，因此文章提出用一种稀疏链接的方式，能够在保证效率的同时有更好的效果。具体而言就是原本是密集连接的feature connection，现在变成相隔2的指数的跨层连接，公式写出来就是</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>=</mo><mi>H</mi><mi>i</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><msub><mi>f</mi><mrow><mi>i</mi><mo>−</mo><msup><mn>2</mn><mn>0</mn></msup></mrow></msub><mo separator="true">,</mo><msub><mi>f</mi><mrow><mi>i</mi><mo>−</mo><msup><mn>2</mn><mn>1</mn></msup></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">f_i = Hi([f_{i-2^0},f_{i-2^1},...]),i=1,2,...,L,
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault">i</span><span class="mopen">(</span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.52238em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.235951em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.52238em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.235951em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mclose">]</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">L</span><span class="mpunct">,</span></span></span></span></span></p>
<h3 id="f-number-adaptation-for-maximizing-the-sensor-usage-of-light-field-cameras-icme19"><a class="markdownIt-Anchor" href="#f-number-adaptation-for-maximizing-the-sensor-usage-of-light-field-cameras-icme19"></a> F-number adaptation for maximizing the sensor usage of light field cameras-ICME19</h3>
<p>这篇文章旨在解决两个问题，其一是现有光场相机，无论是1.0还是2.0,都有有效成像面积不足的问题，其二则是现有光场相机都存在vignetting的问题，是成像决定的，它会导致macro pixel的边缘出现颜色不一致的情况。文章提出，这是因为1.0和2.0的设计中，都匹配了透镜的F数，导致macro pixel的堆叠方式是有空隙的，从而使得传感器利用率不够。</p>
<p>文章提出建立传感器利用率(sensor usage, SU)与主透镜的F数之间的函数关系，通过优化函数来得到最适宜的F数，从而搭建新相机，并利用函数得到的像素选择mask来挑选有效的pixel，从而解决vignetting的问题。从文章的实验结果看，其有效的分辨率得到了提升，vignetting现象也有较大的缓解：</p>
<p><strong>图1：实验的数据结果</strong></p>
<p><img src="/2019/08/26/Google-scholar推送概览/190826-F-number1.png" alt="190826-F-number1"></p>
<p><strong>图2：拍摄得到的图像的比较（左为F数匹配的情况，右为文章的结果）</strong></p>
<p><img src="/2019/08/26/Google-scholar推送概览/190826-F-number2.png" alt="190826-F-number2"></p>
<p><strong>图3：Vignetting问题比较</strong></p>
<p><img src="/2019/08/26/Google-scholar推送概览/190826-F-number3.png" alt="190826-F-number3"></p>
<h3 id="deep-slice-interpolation-via-marginal-super-resolution-fusion-and-refinement"><a class="markdownIt-Anchor" href="#deep-slice-interpolation-via-marginal-super-resolution-fusion-and-refinement"></a> Deep slice interpolation via marginal super-resolution, fusion and refinement</h3>
<p>这篇文章是做MRI在z轴方向的插值的，具体方法是用一个参数共享的网络，做x-z方向和y-z方向的插值，得到两个插值得到的cube，然后再用一个融合网络进行进一步的融合，结合横方向与竖方向的信息。在光场上，EPI是否也可以用相同的思路呢？或者说，不同方向的EPI的特性确实不一样，可以达到不同的效果？</p>
<h3 id="an-objective-assessment-method-for-video-stabilization-performance"><a class="markdownIt-Anchor" href="#an-objective-assessment-method-for-video-stabilization-performance"></a> An objective assessment method for video stabilization performance</h3>
<p>这篇文章提出一个客观的衡量视频稳定性的方法，给出的公式很奇怪，反正我是没看明白这公式里头的notation，后面的实验也很奇怪，没有和主观质量进行一个相关性比较，反正总体感觉一般。</p>
<h3 id="omnimvs-end-to-end-learning-for-omnidirectional-stereo-matching"><a class="markdownIt-Anchor" href="#omnimvs-end-to-end-learning-for-omnidirectional-stereo-matching"></a> OmniMVS: End-to-end learning for omnidirectional stereo matching</h3>
<p>全方向的multi-view stereo，文章的系统设计是使用四个超广角的鱼眼相机组成一个全方向的操作台，拍摄全方向的场景照片。这些场景照片通过2D CNN提取feature，然后把它们warp到深度候选值对应的同心圆，得到一系列的warp后的aligned feature volume，通过3D的encoder-decoder来生成全方向的深度值。系统如下图：</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190827-OmniMVS1.png" alt="190827-OmniMVS1"></p>
<p>具体解法如下图：</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190827-OmniMVS2.png" alt="190827-OmniMVS2"></p>
<p>解法相对来说比较常规，是stereo matching的常规操作，不过extend到了3D的球状sweeping上，这是很有新意的地方，让我想到了spherical CNN，球形卷积。</p>
<h3 id="ranksrgan-generative-adversarial-networks-with-ranker-for-image-super-resolution-iccv19"><a class="markdownIt-Anchor" href="#ranksrgan-generative-adversarial-networks-with-ranker-for-image-super-resolution-iccv19"></a> RankSRGAN: generative adversarial networks with ranker for image super-resolution-ICCV19</h3>
<p>17年SRGAN问世，利用生成对抗的思想，加上了MSE、VGG和对抗loss三个loss来得到perceptual非常nice的SR结果。</p>
<h4 id="esrgan"><a class="markdownIt-Anchor" href="#esrgan"></a> ESRGAN</h4>
<p>18年ECCV的PIRM比赛，Chao Dong的小组提出了ESRGAN，也就是EnhanceSRGAN，ESRGAN相比于SRGAN，在三个方面做了改进：</p>
<ol>
<li>网络结构方面，网络结构采用了RCAN提出的residual in residual dense block来代替SRGAN使用的单纯的残差块；</li>
<li>discriminator方面，使用Relativistic average GAN代替原本的vanilla GAN，主要区别在于它的D网络是学习判断一张结果图是否比另一张更像真的，而不是判断一张结果图是真的还是假的，是一种更加放松的方式；</li>
<li>在perceptual loss方面，采用在激活层前面的VGG feature计算distance取代SRGAN中的激活层后的feature，经验表明结果会更好一些。</li>
</ol>
<p>下图是Relativistic average GAN的概念理解图：</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190827-ESRGAN1.png" alt="190827-ESRGAN1"></p>
<p>并且ESRGAN还提出了一个网络插值的概念，训练一个纯PSNR向的网络和一个纯GAN向的网络，利用一个权重参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>来控制插值：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>θ</mi><mi>G</mi><mrow><mi>I</mi><mi>N</mi><mi>T</mi><mi>E</mi><mi>R</mi><mi>P</mi></mrow></msubsup><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><msubsup><mi>θ</mi><mi>G</mi><mrow><mi>P</mi><mi>S</mi><mi>N</mi><mi>R</mi></mrow></msubsup><mo>+</mo><mi>α</mi><msubsup><mi>θ</mi><mi>G</mi><mrow><mi>G</mi><mi>A</mi><mi>N</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">\theta^{INTERP}_G=(1-\alpha)\theta^{PSNR}_G+\alpha \theta^{GAN}_G
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138331em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07847em;">I</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">E</span><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.138331em;vertical-align:-0.247em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight">A</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>可以得到一些综合二者特征的结果，纯GAN虽然很sharp，但是会有一些过于sharp的artifacts，而纯PSNR面向的则是过于smooth：</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190827-ESRGAN2.png" alt="190827-ESRGAN2"></p>
<h4 id="ranksrgan"><a class="markdownIt-Anchor" href="#ranksrgan"></a> RankSRGAN</h4>
<p>这是19年ICCV的oral，Chao Dong的小组提出的RankSRGAN，主要的思想是解决现有的perceptual metric无法求导以至于无法放进网络进行优化的问题，文章通过建立数据集训练ranker来对不同算法得到的结果进行排序，从而得到一个对图像主观质量的排序器，进而通过这个排序器来优化SRGAN的生成。</p>
<p>具体网络架构：</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190828-RankSRGAN.png" alt="190828-RankSRGAN"></p>
<p>总共分为三个stage，第一个是创建pair-wise的SR结果比较，利用特定的视觉质量指标，如PI、NIQE等，第二个部分是利用一个孪生网络（用于pair-wise的比较）以及margin ranking loss来得到一个排序器，最后在恢复的时候加入一个额外的rank loss，让它生成的图像的rank值尽量大，即保证视觉指标尽量高。</p>
<p>这篇文章最大的创新点在于将原本无法求导的视觉质量指标转换成了一个排序问题，绕过了精准模拟视觉指标的这个坎儿，但是这样是否会带来一个问题，那就是margin loss是当<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>m</mi><msub><mi>y</mi><mn>1</mn></msub></msub><mo>&lt;</mo><msub><mi>m</mi><msub><mi>y</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">m_{y_1}&lt;m_{y_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8252079999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>的时候，要让<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">y_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的得分差距尽量的大，忽略了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>m</mi><msub><mi>y</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">m_{y_1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>m</mi><msub><mi>y</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">m_{y_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>数值之间的差距，因为单纯看相对值是相当于忽略了difference的大小而只看值的高低，这个ranker只能分辨better or not而不知道到底有多better，不过rank-content loss是为了让G生成的SR图像的打分尽量的高就行了，所以并不需要知道有多better。那么这样的话，网络的performance就很依赖于rank的数据集，也就是使用的base的SR算法了。</p>
<h3 id="progressive-face-super-resolution-via-attention-to-facial-landmark-bmvc19"><a class="markdownIt-Anchor" href="#progressive-face-super-resolution-via-attention-to-facial-landmark-bmvc19"></a> Progressive face super-resolution via attention to facial landmark-BMVC19</h3>
<p>这篇文章是利用了facial landmark来参与loss的计算，从而使SR结果在landmark显著区域能够有更好的表现。具体做法就不看了。</p>
<h3 id="preserving-semantic-and-temporal-consistency-for-unpaired-video-to-video-translation-mm19"><a class="markdownIt-Anchor" href="#preserving-semantic-and-temporal-consistency-for-unpaired-video-to-video-translation-mm19"></a> Preserving semantic and temporal consistency for unpaired video-to-video translation-MM19</h3>
<p>这篇文章是19的MM，主要想做的是在video2video的过程中，保持语义和时间上的一致性，这里的语义一致性我没太细看，主要是看它怎么做时间一致性的，跟之前那篇learning blind temporal consistency是用的一样的目标函数，看来是脱胎于那篇文章的做法，在这一块没啥太新意的地方。</p>
<h3 id="no-reference-light-field-image-quality-assessment-based-on-spatial-angular-measurement"><a class="markdownIt-Anchor" href="#no-reference-light-field-image-quality-assessment-based-on-spatial-angular-measurement"></a> No-reference light field image quality assessment based on spatial-angular measurement</h3>
<p>这篇文章就是之前师兄跟我提到的，做光场质量评价的，有时间可以仔细看一下。</p>
<h3 id="point-based-multi-view-stereo-network"><a class="markdownIt-Anchor" href="#point-based-multi-view-stereo-network"></a> Point-based multi-view stereo network</h3>
<p>这篇文章在做multi-view stereo的时候，做法与之前的使用cost volume的做法不同，它采用一种coarse-to-fine的结构，利用粗粒度的depth来得到3D的点云，然后使用一个point-based network来refine深度图。</p>
<h3 id="edvr-video-restoration-with-enhanced-deformable-convolutional-networks-cvprw19"><a class="markdownIt-Anchor" href="#edvr-video-restoration-with-enhanced-deformable-convolutional-networks-cvprw19"></a> EDVR: Video restoration with enhanced deformable convolutional networks-CVPRW19</h3>
<p>这篇文章是19年CVPR的NTIRE视频超分竞赛拿奖的工作，主要是用到了两个重要模块：PCD(pyramid, cascading and deformable convolution)和TSA(Temporal and spatial attention)，具体做法可以后续再看了。</p>
<h3 id="multi-view-to-novel-view-synthesizing-novel-views-with-self-learned-confidence-eccv18"><a class="markdownIt-Anchor" href="#multi-view-to-novel-view-synthesizing-novel-views-with-self-learned-confidence-eccv18"></a> Multi-view to novel view: synthesizing novel views with self-learned confidence-ECCV18</h3>
<p>这篇文章是一个不需要3D监督的、从多个视角生成新视角的工作，主要框图如下：</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190828-Multiview.png" alt="190828-Multiview"></p>
<p>输入为几个已知视角的图像和对应的camera pose，通过flow predictor预测到target视角的flow，然后通过warp得到target视角的prediction和confidence，另外一方面，有一个generator可以直接生成target视角的图，最后通过一个aggregation网络借助confidence map来合成这些所有的结果。</p>
<h3 id="dense-view-synthesis-for-three-dimensional-light-field-display-based-on-unsupervised-learning-oe19"><a class="markdownIt-Anchor" href="#dense-view-synthesis-for-three-dimensional-light-field-display-based-on-unsupervised-learning-oe19"></a> Dense-view synthesis for three-dimensional light-field display based on unsupervised learning-OE19</h3>
<p>这篇文章使用一个unsupervise的方式来训练学习dense的光场视角合成，具体方法就不细看了。</p>
<h3 id="robust-depth-estimation-for-multi-occlusion-in-light-field-images-oe19"><a class="markdownIt-Anchor" href="#robust-depth-estimation-for-multi-occlusion-in-light-field-images-oe19"></a> Robust depth estimation for multi-occlusion in light-field images-OE19</h3>
<p>这篇文章主要想解决multi-occlusion条件下的光场深度估计，用的是一些几何上的occlusion建模方式，类似与OCC那篇经典的求深度文章，具体方法就不看了。</p>
<h3 id="image-super-resolution-by-neural-texture-transfer-cvpr19"><a class="markdownIt-Anchor" href="#image-super-resolution-by-neural-texture-transfer-cvpr19"></a> Image super-resolution by neural texture transfer-CVPR19</h3>
<p>这篇文章试图解决基于reference的SR存在的一个问题：过于依赖reference和target image之间的相似性，一旦相似性太低，RefSR的效果就会很差。文章的解法是通过feature level的特征搜索在ref中搜索到想要的内容，然后通过neural style transfer把ref中的内容transfer到target上去。</p>
<h3 id="recovering-realistic-texture-in-image-super-resolution-by-deep-spatial-feature-transform-cvpr18"><a class="markdownIt-Anchor" href="#recovering-realistic-texture-in-image-super-resolution-by-deep-spatial-feature-transform-cvpr18"></a> Recovering realistic texture in image super-resolution by deep spatial feature transform-CVPR18</h3>
<p>这篇文章使用语义分割的结果来辅助SR，其实是利用基于类别的区域分割信息来引导feature的提取和利用，具体做法是将分割的概率图送进一个shared的网络，得到SFT的condition，然后公用给每一个SFT layer，SFT layer则是学习feature的变换，对feature做一个仿射变换传入下一层，从而融入了分割的信息进去。</p>
<h3 id="srobb-targeted-perceptual-loss-for-single-image-super-resolution"><a class="markdownIt-Anchor" href="#srobb-targeted-perceptual-loss-for-single-image-super-resolution"></a> SROBB: Targeted perceptual loss for single image super-resolution</h3>
<p>这篇文章提出的观点是VGG loss这样的perceptual loss应该要有一定的区分性，本质上是人眼对不同物体的观感是不一样的，因此文章的做法是先用分割的方式将图片分为object、background和boundary这三个level（OBB），其中图像边缘，使用low-level的feature来做perceptual loss，图像的纹理则用mid-level的feature来做loss，因为人对纹理的观感更侧重于语义上或者说主观视觉上，而边界则相对而言是更加low-level的信息，用low-level的feature来约束能够有更好的效果。这篇文章总体来说是很有意思的，我觉得可以细读一下。</p>
<h3 id="reduced-reference-quality-assessment-of-light-field-images-tob19"><a class="markdownIt-Anchor" href="#reduced-reference-quality-assessment-of-light-field-images-tob19"></a> Reduced reference quality assessment of light field images-TOB19</h3>
<p>这篇文章是用gt估计得到的disparity和降质的光场得到的disparity之间的失真来衡量光场的失真，相当于是一种reduced reference QA。</p>
<h2 id="20190902"><a class="markdownIt-Anchor" href="#20190902"></a> 2019.09.02</h2>
<h3 id="depth-agmnet-an-atrous-granular-multiscale-stereo-network-based-on-depth-edge-auxiliary-task-aaai2020"><a class="markdownIt-Anchor" href="#depth-agmnet-an-atrous-granular-multiscale-stereo-network-based-on-depth-edge-auxiliary-task-aaai2020"></a> Depth-AGMNet: an atrous granular multiscale stereo network based on depth edge auxiliary task-AAAI2020</h3>
<p>本文针对的是当前stereo matching方法的一些问题，主要是在ill-posed的region上，包括texture-less和edge区域的一些地方，现有的方法都是用stacked 3D conv或者parallel structure（具体是啥也不太懂），或者加入edge或segmentation信息来辅助stereo matching的。文章其实也是这么做的，主要创新点在于提出了用depth edge辅助disparity estimation的思路。</p>
<h3 id="discriminative-video-representation-learning-using-support-vector-classifiers-tpami2019"><a class="markdownIt-Anchor" href="#discriminative-video-representation-learning-using-support-vector-classifiers-tpami2019"></a> Discriminative video representation learning using support vector classifiers-TPAMI2019</h3>
<p>这篇文章很有意思，现有的视频动作识别的方法对短的clip都是生成独立的预测，然后不同的clip经过一个pooling得到最后的预测结果，但其实并不是所有帧都表征action，实际上可能会有一些帧在很多action里都是很common的，简单的pooling相当于在同质化不同帧的表征水平。文章提出使用discriminative pooling，在所有short clip中，至少有一个是能够非常好地表征action的，文章提出使用与action label无关的negative bag，和当前有关的positive bag来训练一个针对当前action的分类界面，从而形成多个分类超平面，对视频进行更好的分类。而这个超平面的参数就是这个视频的一个表征子。（具体的文章也没有仔细看，大致的理解是这样）</p>
<h3 id="global-local-temporal-representations-for-video-person-re-identification"><a class="markdownIt-Anchor" href="#global-local-temporal-representations-for-video-person-re-identification"></a> Global-local temporal representations for video person re-identification</h3>
<p>这篇文章很有意思，是为了Person Re-ID设计了一个global-local temporal representation的网络，用来利用多尺度的时域信息，网络主要是两个大的步骤：空域特征提取和时域信息融合。空域特征提取空间特征，是用了最普通的ResNet50，文章的重点在时域信息的融合上。</p>
<p>主要提出了两个block，一个是Dilated Temporal Pyramid (DTP) convolution，对提取到的多帧空域特征进行时域信息提取，采用的是多尺度的dilated conv，通过这种方式来获取local的时域信息，再通过self-attention来获取global的时域信息。</p>
<p><img src="/2019/08/26/Google-scholar推送概览/190910-ReID.png" alt="190910-ReID"></p>
<h3 id="video-saliency-prediction-using-spatiotemporal-residual-attentive-networks-tip19"><a class="markdownIt-Anchor" href="#video-saliency-prediction-using-spatiotemporal-residual-attentive-networks-tip19"></a> Video saliency prediction using spatiotemporal residual attentive networks-TIP19</h3>
<p>这篇文章是传统two-stream视频high-level方法的改进版本，主要改进在于两个点：</p>
<ol>
<li>将appearance和motion用一个residual cross的连接fusion起来；</li>
<li>一个互补的attention机制；</li>
</ol>
<p>另外，文章还将convLSTM换成了convGRU，后者结构更简单，能用于小型的动态变化的数据。</p>
<h3 id="improved-robust-video-saliency-detection-based-on-long-term-spatial-temporal-information-tip19"><a class="markdownIt-Anchor" href="#improved-robust-video-saliency-detection-based-on-long-term-spatial-temporal-information-tip19"></a> Improved robust video saliency detection based on long-term spatial-temporal information-TIP19</h3>
<p>当视频帧数变多时，现有的一些方法得到的空-时显著性线索的置信度会变小，导致效果不够好。文章设计了一个新网络用于获取长时的依赖信息来增强video saliency的detection效果。</p>
<h3 id="fourier-light-field-microscopy-oe19"><a class="markdownIt-Anchor" href="#fourier-light-field-microscopy-oe19"></a> Fourier light-field microscopy-OE19</h3>
<p>现有的光场显微方法在重建光场的时候会有很多artifacts和很高的计算量，使得光场显微比较难以推向实用，文章提出一个将光场在傅立叶域做处理的方法，包括成像系统和恢复算法，既提高了恢复精度，又让恢复的速度提升了两个量级。在傅立叶域之类的变换域上处理光场之前有一篇PAMI文章也做过，感觉有时间可以研究一下这里头的优势所在。</p>
<h2 id="20190911"><a class="markdownIt-Anchor" href="#20190911"></a> 2019.09.11</h2>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2019/08/26/Google-scholar推送概览/">Google scholar推送概览</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">Zhen Cheng</a></p>
        <p><span>发布时间:</span>2019-08-26, 15:58:34</p>
        <p><span>最后更新:</span>2019-09-11, 11:50:39</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2019/08/26/Google-scholar推送概览/" title="Google scholar推送概览">http://Joechann0831.github.io/2019/08/26/Google-scholar推送概览/</a>
            <span class="copy-path" data-clipboard-text="原文: http://Joechann0831.github.io/2019/08/26/Google-scholar推送概览/　　作者: Zhen Cheng" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2019/08/31/FlowNet-1-0-and-2-0/">
                    FlowNet 1.0 and 2.0
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2019/07/15/Hexo-Yelee主题首页部分文章摘要无法显示/">
                    Hexo Yelee主题首页部分文章摘要无法显示
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#20190826"><span class="toc-number">1.</span> <span class="toc-text"> 2019.08.26</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#two-stream-sparse-network-for-accurate-image-super-resolution-icmew19"><span class="toc-number">1.1.</span> <span class="toc-text"> Two-stream sparse network for accurate image super-resolution-ICMEW19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#f-number-adaptation-for-maximizing-the-sensor-usage-of-light-field-cameras-icme19"><span class="toc-number">1.2.</span> <span class="toc-text"> F-number adaptation for maximizing the sensor usage of light field cameras-ICME19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deep-slice-interpolation-via-marginal-super-resolution-fusion-and-refinement"><span class="toc-number">1.3.</span> <span class="toc-text"> Deep slice interpolation via marginal super-resolution, fusion and refinement</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#an-objective-assessment-method-for-video-stabilization-performance"><span class="toc-number">1.4.</span> <span class="toc-text"> An objective assessment method for video stabilization performance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#omnimvs-end-to-end-learning-for-omnidirectional-stereo-matching"><span class="toc-number">1.5.</span> <span class="toc-text"> OmniMVS: End-to-end learning for omnidirectional stereo matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ranksrgan-generative-adversarial-networks-with-ranker-for-image-super-resolution-iccv19"><span class="toc-number">1.6.</span> <span class="toc-text"> RankSRGAN: generative adversarial networks with ranker for image super-resolution-ICCV19</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#esrgan"><span class="toc-number">1.6.1.</span> <span class="toc-text"> ESRGAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ranksrgan"><span class="toc-number">1.6.2.</span> <span class="toc-text"> RankSRGAN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#progressive-face-super-resolution-via-attention-to-facial-landmark-bmvc19"><span class="toc-number">1.7.</span> <span class="toc-text"> Progressive face super-resolution via attention to facial landmark-BMVC19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#preserving-semantic-and-temporal-consistency-for-unpaired-video-to-video-translation-mm19"><span class="toc-number">1.8.</span> <span class="toc-text"> Preserving semantic and temporal consistency for unpaired video-to-video translation-MM19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#no-reference-light-field-image-quality-assessment-based-on-spatial-angular-measurement"><span class="toc-number">1.9.</span> <span class="toc-text"> No-reference light field image quality assessment based on spatial-angular measurement</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#point-based-multi-view-stereo-network"><span class="toc-number">1.10.</span> <span class="toc-text"> Point-based multi-view stereo network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#edvr-video-restoration-with-enhanced-deformable-convolutional-networks-cvprw19"><span class="toc-number">1.11.</span> <span class="toc-text"> EDVR: Video restoration with enhanced deformable convolutional networks-CVPRW19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-view-to-novel-view-synthesizing-novel-views-with-self-learned-confidence-eccv18"><span class="toc-number">1.12.</span> <span class="toc-text"> Multi-view to novel view: synthesizing novel views with self-learned confidence-ECCV18</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dense-view-synthesis-for-three-dimensional-light-field-display-based-on-unsupervised-learning-oe19"><span class="toc-number">1.13.</span> <span class="toc-text"> Dense-view synthesis for three-dimensional light-field display based on unsupervised learning-OE19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#robust-depth-estimation-for-multi-occlusion-in-light-field-images-oe19"><span class="toc-number">1.14.</span> <span class="toc-text"> Robust depth estimation for multi-occlusion in light-field images-OE19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#image-super-resolution-by-neural-texture-transfer-cvpr19"><span class="toc-number">1.15.</span> <span class="toc-text"> Image super-resolution by neural texture transfer-CVPR19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#recovering-realistic-texture-in-image-super-resolution-by-deep-spatial-feature-transform-cvpr18"><span class="toc-number">1.16.</span> <span class="toc-text"> Recovering realistic texture in image super-resolution by deep spatial feature transform-CVPR18</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#srobb-targeted-perceptual-loss-for-single-image-super-resolution"><span class="toc-number">1.17.</span> <span class="toc-text"> SROBB: Targeted perceptual loss for single image super-resolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduced-reference-quality-assessment-of-light-field-images-tob19"><span class="toc-number">1.18.</span> <span class="toc-text"> Reduced reference quality assessment of light field images-TOB19</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20190902"><span class="toc-number">2.</span> <span class="toc-text"> 2019.09.02</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#depth-agmnet-an-atrous-granular-multiscale-stereo-network-based-on-depth-edge-auxiliary-task-aaai2020"><span class="toc-number">2.1.</span> <span class="toc-text"> Depth-AGMNet: an atrous granular multiscale stereo network based on depth edge auxiliary task-AAAI2020</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#discriminative-video-representation-learning-using-support-vector-classifiers-tpami2019"><span class="toc-number">2.2.</span> <span class="toc-text"> Discriminative video representation learning using support vector classifiers-TPAMI2019</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#global-local-temporal-representations-for-video-person-re-identification"><span class="toc-number">2.3.</span> <span class="toc-text"> Global-local temporal representations for video person re-identification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#video-saliency-prediction-using-spatiotemporal-residual-attentive-networks-tip19"><span class="toc-number">2.4.</span> <span class="toc-text"> Video saliency prediction using spatiotemporal residual attentive networks-TIP19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#improved-robust-video-saliency-detection-based-on-long-term-spatial-temporal-information-tip19"><span class="toc-number">2.5.</span> <span class="toc-text"> Improved robust video saliency detection based on long-term spatial-temporal information-TIP19</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fourier-light-field-microscopy-oe19"><span class="toc-number">2.6.</span> <span class="toc-text"> Fourier light-field microscopy-OE19</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20190911"><span class="toc-number">3.</span> <span class="toc-text"> 2019.09.11</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"Google scholar推送概览　| Zhen's blog　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    
        <section id="comments" style="margin: 2em; padding: 2em; background: rgba(255, 255, 255, 0.5)">
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: '185312dab9e8351756b5',
      clientSecret: '93c807c1c7089a98ca29a343f4d2991e7b517f23',
      repo: 'Joechann0831.github.io',
      owner: 'Joechann0831',
      admin: ['Joechann0831'],
      id: 'Mon Aug 26 2019 15:58:34 GMT+0800'
    })
    gitalk.render('gitalk-container')
  </script>
</section>
    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2019/08/31/FlowNet-1-0-and-2-0/" title="上一篇: FlowNet 1.0 and 2.0">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2019/07/15/Hexo-Yelee主题首页部分文章摘要无法显示/" title="下一篇: Hexo Yelee主题首页部分文章摘要无法显示">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/31/FlowNet-1-0-and-2-0/">FlowNet 1.0 and 2.0</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/26/Google-scholar推送概览/">Google scholar推送概览</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/15/Hexo-Yelee主题首页部分文章摘要无法显示/">Hexo Yelee主题首页部分文章摘要无法显示</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/15/Neural-RGB-D-Sensing-Depth-and-Uncertainty-from-a-Video-Camera/">Neural RGB-D Sensing: Depth and Uncertainty from a Video Camera</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/14/ODE-inspired-Network-Design-for-Single-Image-Super-Resolution/">ODE-inspired Network Design for Single Image Super-Resolution</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/12/hexo中部分公式无法显示的问题/">hexo yelee主题中部分公式无法显示的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/">SelFlow: Self-Supervised Learning of Optical Flow-CVPR19</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/26/Deep-Video-Super-Resolution-Network-Using-Dynamic-Upsampling-Filters-Without-Explicit-Motion-Compensation/">Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Separable-Convolution/">Video Frame Interpolation via Adaptive Separable Convolution</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Convolution/">Video Frame Interpolation via Adaptive Convolution</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/25/Toward-Real-World-Single-Image-Super-Resolution-A-New-Benchmark-and-A-New-Model/">Toward Real-World Single Image Super-Resolution: A New Benchmark and A New Model</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/">Burst Denoising with Kernel Prediction Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/">Light field intrinsics with a deep encoder-decoder network</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/">An Epipolar Volume Autoencoder with Adversarial Loss for Deep Light Field Super-Resolution</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/">Residual Networks for Light Field Image Super-Resolution</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/">EPINet: A Fully Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/博客搭建日志/">博客搭建日志</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019 Zhen Cheng
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style="display:none">
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style="display:none">
                        <span id="page-visit" title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>



<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-142115103-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>