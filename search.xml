<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SelFlow: Self-Supervised Learning of Optical Flow-CVPR19</title>
      <link href="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/"/>
      <url>/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/title.png" alt="title"></p><p>这是CVPR19 paper reading精读任务中的第一篇，Oral Presentation, Best Paper Final List, Top 1%，反正很屌就对了，其实是从他们团队之前AAAI19的一篇文章进化而来的，为了精读这篇CVPR，我读了他们的AAAI19,顺便再go deep读了关于知识蒸馏(knowledge distillation)的两篇稍微古老一点的paper。</p><a id="more"></a><h1 id="NIPS14-Do-Deep-Nets-Really-Need-to-be-Deep"><a href="#NIPS14-Do-Deep-Nets-Really-Need-to-be-Deep" class="headerlink" title="NIPS14: Do Deep Nets Really Need to be Deep?"></a>NIPS14: Do Deep Nets Really Need to be Deep?</h1><h3 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h3><p>知识蒸馏的学术源头大家都说来自于这篇文章，其实更早的源头是06年SIGKDD的Model Compression，模型压缩的核心思想是用有label的数据来训练一个大而且准确的model，然后用没有label的数据经过这个model生成score作为数据的伪label，即合成label，然后用这些label去指导小的mimic model来学习。</p><p>小的mimic model并不是用original的label来训练出来的，而是被训练用于学习大model学习到的函数，说起来有点绕，其实就是大model学习original label，小model学习大model，相当于大的model是小的model的老师，因此这种方式又被称为teacher-student modeling。模型压缩证明了一个问题，那就是小model无法直接学习到的东西，可以通过大model来学习，然后指导小model进行学习，往往会达到和大的model近似、比直接用original label更好的效果。</p><p>换言之，复杂的模型学习到的函数，并没有想像中那么复杂，以至于简单模型无法学习。</p><h3 id="深度网络-vs-浅层网络"><a href="#深度网络-vs-浅层网络" class="headerlink" title="深度网络 vs. 浅层网络"></a>深度网络 vs. 浅层网络</h3><p>NIPS14这篇文章的核心观点在于，现有的很多研究都证明了，深层网络能够比浅层网络达到更高的准确率，那么这个准确率增益得益于深层网络的什么特征呢？是更多的参数，深层网络的层次化结构，还是现有的优化算法和正则更适用于深层网络呢？文章通过实验证明了，浅层的网络可以学习到深层网络能够学到的一样复杂的函数，并且获得近似的accuracy，从而说明了浅层网络相比于深层网络的问题并不在与capacity和表征能力，而是在于训练和正则的方法。</p><p>文章的做法是这样的，在语音分类数据集TIMIT和图像分类数据集CIFAR-10上，用深度网络训练一个高准确率的分类器，使用正常的softmax分类输出$p_k = e^{z_{k}}/\sum_j e^{z_j}$和cross-entropy损失函数。</p><p>在此之后，就是训练浅层网络了，浅层的mimic model训练的时候使用的不是softmax而是logits，也就是深层网络的输出在送入softmax之前的内容$z_k​$，这种方式的好处在于两点：</p><ol><li><p>如果使用概率值也就是softmax的输出，那么如果teacher输出$[2\times 10^{-9}, 4\times 10^{-5},0.9999]​$，那么student模型在学习的时候会忽略前两项，因为它们太小了，但实际上如果关注logits的话，可以看到[10,20,30]，其实差别不大，说明这个input还是对前两类有一定的倾向性的，student学习这种label，会更利于学习teacher模型的表征；</p></li><li><p>（<a href="https://www.zhihu.com/question/50519680/answer/136363665" target="_blank" rel="noopener">取自Wang Naiyang的回答</a>）</p><p><code>传统的分类问题，模型的目标是将输入的特征映射到输出空间给定的某些点中的一个，换言之，就是要将所有的输入图像映射到输出空间的N个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。</code></p></li></ol><p>小结一下就是，将原本hard的分类label变成soft的话，能够提供样本更加丰富的特征信息，有助于model去理解样本的更深入的、更加独特的特征，也就是所谓的类内difference。</p><p>可以看一下在TIMIT上的实验结果的对比，TIMIT上的实验是用将original label直接遗弃替换成teacher model给的label做的，后面的CIFAR-10则是引入了额外的unlabeled data：</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/NIPS14-1.png" alt="NIPS14-1"></p><p>可以看出，使用了teacher model的label做训练的时候，会比使用hard label训练的准确率更高，这就证明了teacher model的有效性，如果在训练的时候，保留一定的不确定信息（这个往往人类是很难给出的），会更有助于训练。</p><p>小结：</p><p>shallow model如果想要达到好的效果，肯定要变得wide，但同时，就会带来优化上的难度和overfit的风险，可以从下图看出，当参数量变大的时候，使用hard的label会导致效果下降。</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/NIPS14-2.png" alt="NIPS14-2"></p><h3 id="Mimic-model可以防止过拟合"><a href="#Mimic-model可以防止过拟合" class="headerlink" title="Mimic model可以防止过拟合"></a>Mimic model可以防止过拟合</h3><p>teacher-student模型给训练的target一个soft的label，实质上是给出了一个从input直接（通过深层模型）得到的target，增加了label赋予样本的信息量，放宽了output的空间，相当于某种形式的正则化，因此从下图可以看出，过拟合被有效地防止了：</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/NIPS14-3.png" alt="NIPS14-3"></p><p>另外，从下图可以看出，浅层模型并不像13年Big neural networks waste capacity说的那样，表达能力不行，它与深层模型相比，是训练和正则化的算法限制了它的performance：</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/NIPS14-4.png" alt="NIPS14-4"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>浅层模型在并行计算大行其道的当下，是一种适用于商业产品的好模型，但是它的各种问题也很显然，那就是在参数量大的时候难以训练，因为矩阵乘法的维度太高，往往得不到好的结果。本文证明了在使用模型压缩技术，也就是teacher-student模型的情况下，能够将浅层模型训练得很好，只要给一个足够准确的老师或者足够多的unlabeled data。这项发现在实际应用中是很有前途的。</p><h1 id="SIGKDD06-Model-Compression"><a href="#SIGKDD06-Model-Compression" class="headerlink" title="SIGKDD06: Model Compression"></a>SIGKDD06: Model Compression</h1><p>既然NIPS14的这篇文章是基于model compression，后面要读的那篇Hinton老爷子的文章，也就是最早提出知识蒸馏的文章，也是基于这篇KDD的，那就先把这篇文章读一下吧，逐本溯源嘛。</p><h3 id="Model-compression"><a href="#Model-compression" class="headerlink" title="Model compression"></a>Model compression</h3><p>所谓的model compression，主要是考虑到了一个问题，基本上最好的有监督学习模型都是很多基础模型的ensemble，ensemble的方法多种多样， 比如bagging, boosting, random forests等等，但是它们带来高准确率的同时也带来了巨大的计算和内存消耗，如果能够“压缩”模型，使得模型在不怎么损失准确率的情况下能够降低计算和内存消耗，岂不美哉？</p><p>文章提出的方案：ensemble网络是用一个相对而言不太大的有label数据集训练得到的，而我们希望用一个单一更加轻量的ANN（那时候deep learning还不怎么火热呢）来得到接近的效果，那么用大量的无label数据送入训练好的ensemble网络，用它来生成伪label，然后让ANN去学习这个伪label，进而达到接近ensemble的效果。</p><p>为什么可以这样做?我个人认为，主要是简单的ANN要么太简单，表达能力不够，要么就是表达能力够但是没有合适的训练或者监督来使它学好，因此一个teacher模型就显得至关重要了，<strong>复杂网络学习到的表征，不一定是简单网络无法学习的</strong>，这也是NIPS14的观点之一。</p><p>优势：</p><ol><li>能够用轻量级的网络去得到高准确率的模型，减少了开销；</li><li>由于大量的无label数据的引入，训练出来的网络能够有效地防止overfit；</li></ol><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><p>从上面的分析可以看出，model compression的重点在于两个，一个是好的ensemble网络，一个是大量没有label的数据，前者提供一个好的label生成模型，后者则提供大量的训练数据。文章提到，某些域的数据是很好获取的，例如text、web、image等，但是有些域的数据可能并不一定好获取，它们的不同属性数据可能比较难从真实数据中获取，那么文章就提出了一个非参数的生成合成数据的方法MUNGE，也提出了两个含参的生成方法RANDOM和NBE，RANDOM是从各个拟合的边缘分布中采样，NBE则是用朴素贝叶斯估计来估计联合分布，而MUNGE作为一个非参方法，是用最近邻样本属性交换或者重采样的方法获取样本的，具体方法就不再赘述。</p><p>给出三个结果图就可以看出这篇文章思路的价值所在了：</p><p>第一个是RMSE和训练数据的关系，有label的训练数据只有4k，所以横坐标从4k开始：</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/KDD06-1.png" alt="KDD06-1"></p><p>从上述结果可以看出MUNGE是三个生成方法中最好的，而且通过这种model compression的方式，确实能够达到用轻量级模型去接近ensemble模型效果的目的。</p><p>第二个是轻量级模型的hidden units个数和RMSE的关系，表现了当轻量级网络的参数量增多时，用model compression方法能够得到越来越好的效果。</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/KDD06-2.png" alt="KDD06-2"></p><p>第三个则是表现数据合成方法的，表明数据合成还是不如真实的unlabeled data（废话）：</p><p><img src="/2019/07/10/SelFlow-Self-Supervised-Learning-of-Optical-Flow-CVPR19/KDD06-3.png" alt="KDD06-3"></p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>这篇文章可以说很有意思，它针对当时ensemble大行其道的情况做了一个很有远见的思考，那就是，ensemble固然可以增强效果，但是却给模型带来了大量的复杂度，导致模型的开销太大，但使用轻量级的网络又训练不出那么好的效果，要么是数据量不够，要么是表达能力不够，咋整呢？Model compression就一定程度上解决了这个问题。</p><p>这个方案也引领我反思了一下目前的SISR方法，大多数的网络方法都喜欢学习完之后用个ensemble，如果SISR需要好的效果同时也需要低开销的话，用一用model compression也就是后来的知识蒸馏，未尝不是个好的方案。</p><p>同时呢，文章也向我们展示了一个问题，那就是，轻量级的网络并不是表达能力不够，而是没有用足够的数据和合适的方案去调教它，14年这篇文章提到一个观点，放在这里也适用，<strong>那就是正常的label都是人工标注的，和数据是没有一个显式的直接的函数关系的，而从复杂模型输出的伪label是从一个显式的函数输出出来的，轻量级的网络尽管是轻量的，但是它也能够更好地学习这种拥有显示输入输出关系的函数</strong>。我们以前都认为轻量级网络学不到东西是它太weak，然而实际上不是小兵底子差，而是长官没教好hhhhh</p><p>另外，NIPS14在此文上的改进就是，将单纯地用高准确率模型生成的伪label替换成了分类网络的logits，放松了label的形式，增大了不确定度，对输入的表达反而增强了，提供了更多的信息，soft的label使得浅层的网络也能够很好地表征input的特征了。</p><h1 id="NIPS15-Distilling-the-Knowledge-in-a-Neural-Network"><a href="#NIPS15-Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="NIPS15: Distilling the Knowledge in a Neural Network"></a>NIPS15: Distilling the Knowledge in a Neural Network</h1><p>这篇文章是三巨头之一的Geoffrey Hinton写的，和NIPS14的思路很像，NIPS14是借助模型压缩来研究深层网络和浅层网络之间的差别，这一篇则是研究怎样用一个单一的模型来达到ensemble多个复杂模型得到的效果。可以说这两篇文章都是启发自KDD06的模型压缩方法，而且这篇NIPS15更是task都和KDD06很接近。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> optical flow </tag>
            
            <tag> data distillation </tag>
            
            <tag> self-supervision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation</title>
      <link href="/2019/06/26/Deep-Video-Super-Resolution-Network-Using-Dynamic-Upsampling-Filters-Without-Explicit-Motion-Compensation/"/>
      <url>/2019/06/26/Deep-Video-Super-Resolution-Network-Using-Dynamic-Upsampling-Filters-Without-Explicit-Motion-Compensation/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/26/Deep-Video-Super-Resolution-Network-Using-Dynamic-Upsampling-Filters-Without-Explicit-Motion-Compensation/title.png" alt="title"></p><p>这篇文章是利用KPN来做视频超分辨率的，是CVPR18的文章。</p><a id="more"></a><h3 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h3><p>文章提出的dynamic upsampling filters其实和KPN是基本一样的思路，不过文章说参考文献参考的是16年NIPS的dynamic filter networks，不晓得这俩有啥区别，从算法上看是几乎没什么差别的。文章提出的算法是这样的，用dynamic upsampling filters生成filter，代替deconv等操作对input frame做upsample，然后用另一个网络对所有输入帧提取特征之后求一个residual，增强SR之后的input frame的细节。</p><h4 id="Dynamic-upsampling-filters"><a href="#Dynamic-upsampling-filters" class="headerlink" title="Dynamic upsampling filters"></a>Dynamic upsampling filters</h4><p><img src="/2019/06/26/Deep-Video-Super-Resolution-Network-Using-Dynamic-Upsampling-Filters-Without-Explicit-Motion-Compensation/1.png" alt="1"></p><p>将所有输入帧送进dynamic filter generation network生成卷积核，这个卷积核数量是$r^2HW$个，也就是SR之后的图像的pixel数量，亦即每个output的pixel都对应着一个kernel，然后对LR的input frame，每个pixel上都有对应的$r^2$个kernel，用于卷积并且生成HR上对应位置的pixel。</p><h4 id="整体网络"><a href="#整体网络" class="headerlink" title="整体网络"></a>整体网络</h4><p><img src="/2019/06/26/Deep-Video-Super-Resolution-Network-Using-Dynamic-Upsampling-Filters-Without-Explicit-Motion-Compensation/2.png" alt="2"></p><p>整体网络框架如上图，两个重要的网络是共享参数的，只在后续生成想要的东西的时候进行另外的卷积，相当于是提取feature部分共享，后面分支出来了。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>文章的算法上，是用所有帧生成kernel来对input的LR进行卷积得到一个HR的初步结果，然后用所有帧生成细节来进一步恢复。这里和之前的KPN不一样的地方在于，之前的KPN会对每一个使用到的帧都采用学习到的kernel，造成了非常大的计算资源消耗，这里只用learn到的kernel去学一个初始化的结果，然后用其他的帧来学习细节。</p><p>尽管这样做会更轻量级一点，但是不同帧之间互补的细节信息正是通过对齐和fusion得到的，这里直接通过一个全卷积网络得到细节信息，我觉得还有点不够完美，这里学习到的kernel只用在了input center frame上，所谓的隐式的motion compensation压根就看不出来，也许是太隐式了？因为文章后面花了不少的篇幅去证明他们对temporal信息的利用能力，所以暂且认为它非常非常隐式地利用到了时域的variant特性，但是学习残差这一块直接怼进网络里去实在有点不好理解。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> super resolution </tag>
            
            <tag> video </tag>
            
            <tag> dynamic filter network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Video Frame Interpolation via Adaptive Separable Convolution</title>
      <link href="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Separable-Convolution/"/>
      <url>/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Separable-Convolution/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Separable-Convolution/title.png" alt="title"></p><p>这篇是ICCV17的文章，是前一篇CVPR17的增强版，title也只比前作多了一个词，tql。</p><a id="more"></a><p>这篇文章解决了前一个文章没能解决的问题，因为2D的kernel size必须很大才能处理大的motion，所以无法在这种大size的kernel下直接预测全图per-pixel的kernel，只能pixel by pixel地预测，这就导致了算法的复杂以及最后结果无法加上全图的loss等问题。因此文章提出用1D的kernel来代替2D的kernel，将kernel的复杂度从$K^2$变成$2K$，进而可以实现全图的输出。</p><p><img src="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Separable-Convolution/architecture.png" alt="architecture"></p><p>输出的1D的kernel有两个，一个是horizontal的，一个是vertical的，二者进行外积即可得到一个完整的2D kernel，这种情况下，kernel size可以进一步设大，增强网络对大motion的处理能力。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这篇文章就更接近前面两篇KPN的方法了，都是用一个类似于AE的结构生成全图的per-pixel的kernel来做卷积，不同的是这里为了减少存储量，把2D kernel分解成了两个1D的，还是有点意思的。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kernel prediction networks </tag>
            
            <tag> video frame interpolation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Video Frame Interpolation via Adaptive Convolution</title>
      <link href="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Convolution/"/>
      <url>/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Convolution/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Convolution/title.png" alt="title"></p><p>这是CVPR17的文章，使用KPN来做视频插帧的。</p><a id="more"></a><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><h4 id="KPN"><a href="#KPN" class="headerlink" title="KPN"></a>KPN</h4><p>文章的算法思想也是比较直接地使用KPN，如下图：</p><p><img src="/2019/06/25/Video-Frame-Interpolation-via-Adaptive-Convolution/1.png" alt="1"></p><p>对于输出的$(x,y)$坐标位置，取输入两帧以$(x,y)$为中心的大patch R1和R2,用它们作为输入得到一个kernel，然后用这个kernel和以$(x,y)$为中心的两个小patch P1和P2进行卷积，得到输出帧的$(x,y)$位置的结果。因此，网络的思想也是每个pixel都要生成一个kernel，但是就结构而言，和之前的burst denoising以及realSR有不同的地方：</p><p><em>burst denoising用一个encoder-decoder的结构，使得输出的feature map大小和输入的是一样的，实现了per-pixel的kernel prediction，而realSR用的是一个shuffle downsample + shuffle upsample的方式，也使得输出的feature map和输入的大小一样，这篇文章用的是一个纯downscaling的网络，对一个$6\times79\times79$的输入，输出是一个$41\times 82\times 1\times1$的kernel，因此对一张$h\times w$的输入图像，pixel-wise的实现需要将网络前向传播$h\times w$次，这无疑是非常耗费时间的。</em></p><h4 id="Shift-and-stitch"><a href="#Shift-and-stitch" class="headerlink" title="Shift-and-stitch"></a>Shift-and-stitch</h4><p>鉴于上述区别，文章提出使用shift-and-stitch方法来实现，shift-and-stitch源自于semantic segmentation，由于CNN网络的下采样，使得网络的输出是coarse的，无法实现输入$N\times N$，输出也是$N\times N$，想要得到pixel level的结果的话，其中一个方法就是shift-and-stitch。</p><p>假设降采样因子为$s$，那么将input map进行平移，偏移量为$(x,y)$，其中$x,y \in {0,1,…,s-1}$，这样就可以得到$s^2$个input，通过前向传播，得到$s^2$个output，再经过类似于sub-pixel convolution的交织过程，就可以获得结果了。</p><p>文章这里的做法也是这样，因为downsample了三次，每次都是以2为倍数，所以对输入图像shift 64次，经过64次的前向传播，得到每个位置上的kernel，进而进行卷积。至于位置和shift的offset的关系，这个就不再细述，关于shift-and-stitch以及相关的方法，可以参考<a href="https://zhuanlan.zhihu.com/p/56035377" target="_blank" rel="noopener">Shift and stitch理解</a>。</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>loss函数是intensity loss和gradient loss的加权，gradient loss的设计是先对输入两帧算gradient，然后与计算出来的kernel进行卷积，与GT的gradient计算loss，这是因为kernel K是用于一个输出pixel的，所以无法用来post地计算梯度。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>文章的优点很明显了，在optical flow之类的matching算法的痛点（occlusion, motion blur和lack of texture）上进行研究，提出使用KPN来对matching和pixel synthesis一起操作，同时使用shift-and-stitch的方式，不像之前的网络那样使用类似于AE的方式，可以减小测试端各个level的feature map，至于运算效率如何，没有实验就不太清楚了。</li><li>文章也有一些问题，第一个就是设定的$41\times 41$大小的kernel导致网络只能处理41个pixel以内的motion，超过这个大小的motion就会出现blur的情况，这和17年ICCV的EPICNN一样，需要一个参数来控制motion的最大值，这无疑为网络的泛化增加了很大的限制。一种解决方案是使用multi-scale的方式，类似于光流估计里用的那样，另外其他的更加elegant的方式还有待发掘（或者是我还没读到）。</li><li>这是一个pixel by pixel设计的网络，loss function也是per-pixel计算的，每一次前传都只出来一个预测的pixel，使得网络比较难添加全图或者patch-wise的loss，例如一些perceptual的loss。但没有办法做全图的原因也很明显，因为要handle比较大的motion，必须要比较大的kernel，不像burst denoising和realSR一样不需要那么大的kernel，大的kernel如果再乘以全图的分辨率，需要预测的kernel的输出就会超级大，因此才想出这么个办法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kernel prediction networks </tag>
            
            <tag> video frame interpolation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Toward Real-World Single Image Super-Resolution: A New Benchmark and A New Model</title>
      <link href="/2019/06/25/Toward-Real-World-Single-Image-Super-Resolution-A-New-Benchmark-and-A-New-Model/"/>
      <url>/2019/06/25/Toward-Real-World-Single-Image-Super-Resolution-A-New-Benchmark-and-A-New-Model/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/25/Toward-Real-World-Single-Image-Super-Resolution-A-New-Benchmark-and-A-New-Model/title.png" alt="title"></p><p>这是达摩院、大疆和香港理工大学一起做出来的一个工作，这个数据集的一部分还被用在了NTIRE19的真实图像超分辨率竞赛上。看这篇文章一个原因是它用了KPN，另一个原因也是它做的方向这一年刚好雨后春笋一样冒了一大堆，包括Chang的工作。</p><a id="more"></a><h3 id="Formation"><a href="#Formation" class="headerlink" title="Formation"></a>Formation</h3><p>真实图像分辨率降质的模型其实一直比较有争议，我个人也觉得可以从很多角度思考，最直接的当然就是用不同的焦距来拍摄同一个场景，文中给出了一个比较精简的推导：<br>$$<br>\frac{1}{f} = \frac{1}{u} + \frac{1}{v}<br>$$<br>上式是透镜成像公式，$u$是物距，$v$是像距，所谓的放大或者缩小系数，可以看成是像的大小相对于物体大小的比例：<br>$$<br>M = \frac{h_2}{h_1} = \frac{v}{u}<br>$$<br>当物体大小$h_1$和物距$u$固定时，物距$u$往往比相机的焦距大多了，因此考虑到$u \gg f$并且联立(1)(2)两式，可以得到：<br>$$<br>h_2 = \frac{f}{u-f}h_1\approx \frac{f}{u}h_1<br>$$<br>因此，像的大小可以近似为与焦距成正比。文章基于此结论，进行了数据集的采集和配准工作，这里不再赘述。</p><h3 id="SR网络"><a href="#SR网络" class="headerlink" title="SR网络"></a>SR网络</h3><p>可以想见的是，上述降质模型里面，得到的LR图像与HR图像之间的降质关系（即blur kernel）会跟随物距而改变，因此采用KPN这种结构会更有效一些，所以文章提出的网络结构如下：</p><p><img src="/2019/06/25/Toward-Real-World-Single-Image-Super-Resolution-A-New-Benchmark-and-A-New-Model/network.png" alt="network"></p><p>金字塔架构是因为KPN要生成$k\times k\times w \times h$这么大的map然后做卷积，如果kernel很大，占用的存储量很大，kernel小则效果不够好，不能够处理更大的邻域，所以文章采用一个金字塔架构，无可厚非。KPN结构就是我们之前在burst denoising里看到的一样的结构，只不过是没有那个N而已。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>可以看出，这里对realSR的LR-HR pair对的建模是与Chang师兄不太一样的，但是殊途同归，不同的角度分析得到的结果是近乎一致的。</li><li>对于同一个问题，如果能够有更多不一样或者更深层次的思考，会更有价值一些。</li><li>类似的工作还有两篇：一篇是CVPR19的zoom to learn, learn to zoom，另一篇是TPAMI还没publish的Toward bridging the simulated-to-real gap: benchmarking super-resolution on real data。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> super resolution </tag>
            
            <tag> kernel prediction networks </tag>
            
            <tag> real-world SISR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Burst Denoising with Kernel Prediction Networks</title>
      <link href="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/"/>
      <url>/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/1.png" alt="1"></p><p>这是18年CVPR利用KPN做burst去噪的文章，作者列表里赫然出现了Ren Ng大佬，有、东西～</p><a id="more"></a><h3 id="主要问题"><a href="#主要问题" class="headerlink" title="主要问题"></a>主要问题</h3><p>在去噪领域，多张图像的去噪往往可以利用求平均的方式，但是在使用手持相机的连拍图像里，涉及到相机motion或者场景motion的时候，直接求平均会带来很大的blur效果，去噪效果很差。文章提出使用空间可变的kernel来对齐各个图像并且做去噪，同时还提出了使用退火loss函数的方法。</p><p>拍照过程中的噪声分为两种，一个是shot noise，可以用Poisson过程来模拟，另一个是read noise，相机将raw data读取的时候的噪声，用一个高斯过程来模拟：<br>$$<br>x_{p} \sim \mathcal{N}\left(y_{p}, \sigma_{r}^{2}+\sigma_{s} y_{p}\right)<br>$$<br>其中$x_p$是带噪声的像素值，$y_p$是真实的intensity，对每张image，只要sensor gain是一致的，$\sigma _s$和$\sigma _r$就是确定的。真实图像连拍往往包括手抖动的motion和场景motion，这些都会影响去噪算法的处理。</p><p>burst denoising问题的定义：使用所有的图像对reference图像进行denoise。</p><h3 id="合成数据集"><a href="#合成数据集" class="headerlink" title="合成数据集"></a>合成数据集</h3><p>文章提出，为图像恢复任务收集真实数据集是个很有挑战性的事情，因为算法效果会受限于imaging system的能力，恢复出来的效果不会超过使用的imaging system。因此，文章使用openimage这个数据集提供的高清单帧图像，自行设计了模拟连拍的数据集生成方式（具体参见论文），并且在相机的参数曲线中随机挑选参数用于生成训练集，最后通过公式(1)来生成带噪声的连拍图像。</p><p><strong>可以看出这里的噪声图像和我们平常理解的加性、乘性噪声都不太一样</strong></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>OK，接下来就是我读这篇文章想要get的最重要的点：网络结构的设计。</p><p><img src="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/Architecture.png" alt="Architecture"></p><p>文章用的是一种encoder-decoder的结构模式，输出是一个$K^2N$个通道的feature，feature大小与input的burst一样大，然后它们被reshape成每个pixel上都有N个$K\times K$大小的线性filter，输出需要经过如下的运算：<br>$$<br>\hat{Y}^{p}=\frac{1}{N} \sum_{i=1}^{N}\left\langle f_{i}^{p}, V^{p}\left(X_{i}\right)\right\rangle<br>$$<br>其中$\hat Y ^p$指的是p点上的输出值，$f_i^p$是指第i张image上p点对应的$K\times K$的kernel，$V^p(X_i)$是第i张image上p点的$K\times K$的邻域，所以生成的kernel实际上是per-pixel的卷积核。</p><h3 id="Loss上的改动"><a href="#Loss上的改动" class="headerlink" title="Loss上的改动"></a>Loss上的改动</h3><p>文章提出的loss是L2距离和梯度的L1距离（basic loss），但是实验观察到直接优化这个loss的话，网络会比较快地收敛到局部最优，具体体现的现象就是只有reference帧的filter是非0的。</p><p>作者提出主要的原因是multi-image的alignment和denoising是比单张的更难的，basic loss只会激励训练去考虑参考帧而不考虑其他帧的影响，所以才会导致网络快速收敛到局部最优。</p><p>鉴于此，文章提出退火loss：</p><p><img src="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/TimeVaryingLoss.png" alt="TimeVaryingLoss"></p><p>其中$\beta$是一个超参，$0&lt;\alpha&lt;1$也是个超参，t则是代表iteration的次数，当$\beta \alpha^t \gg 1$时，第二项占据主导地位，说明网络在尝试把每一帧都独立地对齐到参考帧，作为使用basic loss的一种预训练，当t越来越大时，$\beta \alpha^t​$趋近于0，从而前一个basic loss占据主导，网络学习着重新给出filter，使得对齐更好的帧可以对结果产生更好的影响，而对齐更差的帧的影响则被削弱。</p><p>个人感觉，本质上退火loss采用的思想是在优化的前期让网络去优化独立的align，使得各个帧能够具备初步的align能力，然后再使用basic loss去fusion和优化，这种思想和我们CSVT的EnhanceCNN想法是比较一致的，不同的就是我们是完全分开做的，他们则是更加巧妙地使用了退火loss把两步并做了一步。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>网络输入上，文章所提的网络除了输入burst，还将逐点估计的噪声水平（利用设定的相机参数$\sigma_s$和$\sigma_r$估计出来的）作为一个额外的channel输入进来了，这是一个比较有意思的想法：将成像过程中的参数或者与参数相关的东西作为额外的输入，往往有可能会有更好的效果；</li><li>网络结构上，通过生成per-pixel的卷积核，网络对align+去噪这一过程的模拟能力会更加优秀，因为得到的是一个滤波器组，会提供spatial-variant以及input-variant的kernel，这个input-variant不仅仅指输入的burst，还有输入的参数，这个很重要，会使得网络具备对不同条件下成像的burst的鲁棒性，文章的实验也证明了这一点；</li><li>退火loss的设计很巧妙，将我们正常思维中需要分步做的东西一步做到了，而且在local minimum的时候的分析也非常准确，是非常值得学习和借鉴的；</li><li>在这篇文章里，burst的不同位置的噪声和motion都是不一致、不均匀的，因此这就应和了KPN的优势：当要处理的input各个区域的降质或者变形不是一致的时候，可以用这种per-pixel的kernel来做，而且这里的kernel还是根据输入图像生成出来的，具有input variant的特性；</li><li>处理这种局部的形变的话，deformable conv也很有效，这两者的区别在哪里呢？是个值得深思的问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> denoising </tag>
            
            <tag> kernel prediction networks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Light field intrinsics with a deep encoder-decoder network</title>
      <link href="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/"/>
      <url>/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/1.png" alt="1"></p><p>这是承接上一篇文章读的，<a href="http://Joechann0831.github.io/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/">上一篇文章</a>是CVPRW19的利用3D的encoder-decoder加上GAN loss做SR的工作，它的结构其实比较奇怪，最后加上GAN loss的操作也有点莫名其妙，这篇CVPR18正会的文章就好多了。</p><a id="more"></a><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>文章的主要思想其实靠一张图就能看明白了，这也是我之前一直没有读这篇文章的原因，觉得看完图就懂了:</p><p><img src="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/Framework.png" alt="Framework"></p><p>EPI volume通过一个encoder编码到非常低维的空间，然后通过无监督方式训练AE得到表达、通过有监督的训练方式来得到disparity和intrinsic的两个分量。</p><p>一些小细节：</p><ol><li>输入是水平和垂直两个方向的EPI volume，共享网络参数，在disparity encoder之前concat在一起；</li><li>由于diffuse和specular相加是要等于LF的，所以在最后接近输出层时把二者的feature共享了；</li></ol><h3 id="Residual-block的设计"><a href="#Residual-block的设计" class="headerlink" title="Residual block的设计"></a>Residual block的设计</h3><p>文章还提出了它设计的encoder-decoder的res-block，与CVPRW19的是一样的结构，只不过deconv那里没有bicubic上采样这么神奇的操作而已：</p><p><img src="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/Res-block.png" alt="Res-block"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>总的来说，CVPRW19的文章结构上是沿袭了这篇18年的思想，只不过为什么用3D-2D这样的结构代替3D-3D的结构还是有点奇怪，如果单纯是为了结构上比较精简，倒也可以理解了。那么还有两个不太理解的地方就是，为什么要用GAN的loss，如果改用MSE loss比较PSNR和SSIM是否会更合理一些？</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> depth estimation </tag>
            
            <tag> intrinsic decomposition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Epipolar Volume Autoencoder with Adversarial Loss for Deep Light Field Super-Resolution</title>
      <link href="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/"/>
      <url>/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/1.png" alt="1"></p><p>这是19年CVPR NTIRE Workshop上的文章，也是做光场超分辨率的，而且还是第一次见到用生成对抗的loss来做光场的SR，应该挺有意思的。</p><a id="more"></a><p>看完之后，先讲讲优点吧。首先我觉得处理的问题是比较有创新性的，在给定非常少的view的情况下，能够同时做角度和空间的恢复，用到了3D的基于GAN的AE，见下图：</p><p><img src="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/2.png" alt="2"></p><p>输入是三个图中红色或者绿色框标记的view，经过一个3D的encoder得到一个$3\times 3\times 3$的latent表达，再经过9个2D的decoder，出来九个通道的角度SR结果，然后再经过两个(for 4x)或者一个(for 2x)空间超分辨率的网络，做到角度和空间上的超分辨率，最后用一个DiffWGAN的discriminator做判别器。</p><p>但是这里我有几个小疑问：</p><ol><li>为什么不用3D的encoder-decoder直接出来结果，而要用这种方式？节约解码端的计算资源？</li><li>最后为什么要用GAN？众所周知，GAN的discriminator是倾向于视觉质量的，目前光场的SR还没有到需要研究视觉质量的地步，即使这个判别器可能是为了判别输出的光场是否像真实的光场（不仅空间上很好，而且维持了很好的角度一致性），空间和角度一致性无法解耦也会导致空间恢复结果往perceptual靠拢。所以我觉得用GAN不是个特别好的主意，文章中说的是为了增强重建的光场的sharpness，引入了WGAN loss用来惩罚GT和estimation的角度和空间的导数之间的差，为什么不用MSE loss呢？用导数的MSE loss不也可以吗？</li></ol><p>文章的encoder-decoder的设计是参照他们之前在CVPR18里面发表的Light field intrinsics with a deep encoder-decoder network的文章，网络中设计了一些encoder res-block和decoder res-block，主要形式如下：</p><p><img src="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/3.png" alt="3"></p><p>为啥要用Bicubic interpolation呢。。。这个的反传很复杂，而且好像还挺慢的，为啥不用bilinear？</p><p>最后的实验结果和14年PAMI、Bilinear、GB、SRGAN都比较了，但是这个比较看着挺混乱的，不晓得它到底在看重哪个指标，速度、accuracy、visual quality，总得有那么一两个吧？貌似挺混乱的。</p><p>不过这篇文章里面提到他们HCI实验室又做了一个新的数据集，大概就是之前说到的光场合成数据集，用于大型的深度学习任务的，目前有750个光场数据，之前他们在CVPR18提的那个数据集好像才将近200个。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Residual Networks for Light Field Image Super-Resolution</title>
      <link href="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/"/>
      <url>/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-1.png" alt="20190619-1-1"></p><p>这篇是CVPR19的做光场超分辨率的文章，是光场空间超分辨率这三四年以来第一次出现在顶会正会里面。其特点是恢复的accuracy很高，PSNR相比于SOTA高很多。</p><a id="more"></a><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>文章的网络结构没什么特别有新意的地方，跟EPINet是几乎一样的结构：</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-2.png" alt="20190619-1-2"></p><p>可以看出来，同样是multi-stream的结构，只不过加上了central view的分值提供residual的相加项而已，其实本质上是SR方法常用的技巧——把想要SR的view再以更shallow的方式加回去（数据处理不等式）。</p><h2 id="迁移到整个光场的SR"><a href="#迁移到整个光场的SR" class="headerlink" title="迁移到整个光场的SR"></a>迁移到整个光场的SR</h2><p>从上文可以看出，这种网络结构是和我们的CSVT一样，一次只能做一个view，文章提出使用一种灵活的策略来对整个光场做SR，其实主要就是针对不同角度位置来训练不同的网络，利用不同的view间信息：</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-3.png" alt="20190619-1-3"></p><p>以$7\times 7$的view为例，对central view，就用$7\times 7$的策略，对边上的view，根据其位置采用不同的策略，最后构成整个LF的恢复策略，如图中(e)所示。神奇的是，这种方式即使拿四个view，效果也很好（图中展示的是mona数据集的结果）。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="合成数据集"><a href="#合成数据集" class="headerlink" title="合成数据集"></a>合成数据集</h3><p>合成数据集的实验当然是HCI数据集了，其实验结果有一些可圈可点的地方，第一，除了它自己的结果，其他实验结果全是从LFNet的文章里面抄来的，第二，LFNet的文章里，bicubic的结果是不对的，但是我复现它的结果发现LFNet本身的结果确实就是那个数值。以下是合成数据集上的实验结果：</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-4.png" alt="20190619-1-4"></p><h3 id="真实数据集"><a href="#真实数据集" class="headerlink" title="真实数据集"></a>真实数据集</h3><p>文章的一个特点是实验做得比较完全，虽然网络结构确实没什么新意，从下表可以看出，实验结果确实很好，至于在现有的一些更好的SR方法里面是什么样的，也很难讲，例如TIP18的SAS和4D CNN，而且GB的效果在真实数据集里面这么好其实很让人惊讶，因为GB是基于matching的，真实数据集的暗角效应很影响matching，所以其实我是有点小疑问的，不过除了Lytro那个数据集，其他的要么我没有做实验，要么它的测试集划分我不晓得，所以也不好讲，毕竟GB有些时候效果也不错。</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-5.png" alt="20190619-1-5"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总而言之，这篇文章的网络结构，在利用光场多维数据上，想法是和EPINet一样的，那么idea上的问题也比较明显，没有用到所有的view，但是应该是把用到的view之间的关系用好了，所以效果很好。这说明光场SR的空间还是很大的，如果能够有一些方法把光场4D的信息很好地用上，相信效果会很好。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EPINet: A Fully Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images</title>
      <link href="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/"/>
      <url>/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/20190616-1.png" alt="20190616-1"></p><p>这是18年CVPR计算成像板块的一篇poster文章，因为网络结构和19年CVPR做光场SR的很像，所以在这里先读一下这篇，看看二者到底有什么区别。值得一提的是，这篇文章的团队是LFCNN的团队。</p><a id="more"></a><p>这篇文章的主要贡献有两个点：一个是网络结构，采用了一个multi-stream的网络结构，从不同的方向来获取EPI的信息，另一个就是它针对光场深度估计数据集太小做的一个数据增强，提出了一些light field specific的增强方法。</p><h2 id="Multi-stream网络结构"><a href="#Multi-stream网络结构" class="headerlink" title="Multi-stream网络结构"></a>Multi-stream网络结构</h2><p>网络结构图：</p><p><img src="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/20190618-2.png" alt="20190616-2"></p><p>最理想的利用EPI的方法，当然是从EPI中辨别出各个方向的极线，然后计算其斜率得到视差和深度，例如15年PAMI的结构张量方法，但是这种方法是一种特征检测+优化的方法，不使用于深度学习，因此，简化一下，从四个整数正切值的方向获取SAI的stack，分成不同的stream送入网络，将得到的特征concat在一起，再用8个conv block预测最后的disparity map。</p><p><strong>（感觉看完这几篇文章得再总结一下别人是怎么利用高维数据信息的，例如高维的卷积、一些独特设计的卷积结构等，我总觉得这其中有可以挖掘的地方。）</strong></p><h2 id="针对光场深度估计的数据增强方法"><a href="#针对光场深度估计的数据增强方法" class="headerlink" title="针对光场深度估计的数据增强方法"></a>针对光场深度估计的数据增强方法</h2><p>文章提出了很多数据增强方法：变化central view、空间旋转、resize所有view、翻转所有view、色彩调整、直方图拉伸等等。针对空间旋转，不是简单地旋转空间维度就完事了，见下图：</p><p><img src="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/20190618-3.png" alt="20190616-3"></p><p>在旋转90度之后，可以看到横方向的SAI之间的视差已经从原本的横方向变成了纵方向，所以应该要把它送进纵方向的网络里面去，而且label也要对应地旋转。</p><p>resize的话，也是要对视差的值进行数值上的resize的。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>针对这篇文章，去年我看的时候并不觉得其网络结构有什么特别特殊的，但是今年CVPR有人拿它来做SR，效果还贼好，这就很尴尬了。。。一点想法：</p><ol><li>这种方法确实能够利用2D的卷积来比较不错地获取不同角度维的信息，但有一个缺陷，那就是它只能用到0、45、90、135这四个方向上的view，其他位置的view无法在multi-stream中被利用到，因此，如何充分地将所有的view之间的关系（这里所述的关系应当指的是角度上的邻接关系和不同角度的view之间的空间关系）用上仍然是一个值得思考的问题。但是不得不承认这种方式可以比较简单地利用到光场的角度维信息，尽管并不完全；</li><li>文章为了针对小的baseline，用的是$2\times2$的卷积核，其实很少见，但是文章没有比较其他的核大小，所以无法说明是否真的有效。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> depth estimation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建日志</title>
      <link href="/2019/06/15/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97/"/>
      <url>/2019/06/15/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<p>鼓捣了两天多的时间，终于把博客初步搭建好了，功能基本上都OK了，如果还想要别的（比如google analytics），就以后再加啦。</p><a id="more"></a><h1 id="关于使用jekyll还是hexo"><a href="#关于使用jekyll还是hexo" class="headerlink" title="关于使用jekyll还是hexo"></a>关于使用jekyll还是hexo</h1><p><a href="https://jekyllrb.com" target="_blank" rel="noopener">jekyll</a>和<a href="https://hexo.io" target="_blank" rel="noopener">hexo</a>是两个比较常用的静态网页框架，但是二者的体验是完全不一样的。这两天半的时间我主要是花在了jekyll上，使用的主题说明文档虽然很详细，但是涉及到的网页前端的底部知识太多了，依赖环境ruby也老是出问题，搞了老半天，搞了个半成品，地址在<a href="Joechann0831.github.io">失败作品</a>，只有简单的功能，因为复杂的功能都没加成功T_T</p><p>hexo就不一样了，它的开发文档很详细，而且操作很简单高效，加上用户群体多，社区体验很好，很多问题都能找到解决方案，因此使用hexo还是很好的。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><p>jekyll的搭建步骤就不详述了，太麻烦，而且我也没完全成功，这里主要记录一下使用hexo搭建博客的过程吧。</p><p>具体步骤是参考了<a href="https://blog.csdn.net/Gage__/article/details/80302471" target="_blank" rel="noopener">基于Ubuntu搭建Hexo个人博客</a>，这里加上了一些自己遇到的问题。</p><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><h3 id="1-安装Git"><a href="#1-安装Git" class="headerlink" title="1. 安装Git"></a>1. 安装Git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git-core -y</span><br></pre></td></tr></table></figure><h3 id="2-安装Node-js"><a href="#2-安装Node-js" class="headerlink" title="2. 安装Node.js"></a>2. 安装Node.js</h3><p>因为我在之前捣鼓jekyll的时候已经把这俩装好了，所以前面参考的博客里面的那三句话能不能用我也不晓得，就不记录了。</p><h2 id="二、-安装Hexo并初始化模板"><a href="#二、-安装Hexo并初始化模板" class="headerlink" title="二、 安装Hexo并初始化模板"></a>二、 安装Hexo并初始化模板</h2><h3 id="1-安装npm"><a href="#1-安装npm" class="headerlink" title="1. 安装npm"></a>1. 安装npm</h3><p>Hexo的相关依赖和包使用npm来安装的，因此要先安装npm，使用淘宝的源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><h3 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="2. 安装Hexo"></a>2. 安装Hexo</h3><p>创建一个自己的网页文件夹，例如我的是ZhenChengUSTC.github.io文件夹，cd到改文件夹下，执行下述命令安装Hexo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p><strong>注意使用上述两个npm命令时，最好加上sudo权限，否则会出现权限不足的问题</strong></p><h3 id="3-初始化Hexo文件夹"><a href="#3-初始化Hexo文件夹" class="headerlink" title="3. 初始化Hexo文件夹"></a>3. 初始化Hexo文件夹</h3><p>在该文件夹下初始化Hexo的模板：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>此时如果hexo安装所需的依赖没有安装，它会自动安装，只需要耐心等待。</p><h3 id="4-预览Hexo网页"><a href="#4-预览Hexo网页" class="headerlink" title="4. 预览Hexo网页"></a>4. 预览Hexo网页</h3><p>init结束之后，文件夹底下就已经有一个名为landscape的模板了，比较丑，但是我们可以预览一下确认Hexo安装是否正确：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure><p>hexo server默认端口是4000,如果端口被占用，可以更改端口或者解除占用，参考<a href="https://blog.csdn.net/yucicheung/article/details/79535282" target="_blank" rel="noopener">Ubuntu下用hexo搭建github博客</a>，这里不再赘述。</p><h2 id="三、更改主题"><a href="#三、更改主题" class="headerlink" title="三、更改主题"></a>三、更改主题</h2><p>初始的主体不太好看，我选择了yelee主题，这个主体比较符合我的审美而且有完整的<a href="http://moxfive.coding.me/yelee/" target="_blank" rel="noopener">中文说明文档</a>，很方便，具体步骤：</p><h3 id="1-git-clone主题文件"><a href="#1-git-clone主题文件" class="headerlink" title="1. git clone主题文件"></a>1. git clone主题文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:MOxFIVE/hexo-theme-yelee.git themes/yelee</span><br></pre></td></tr></table></figure><h3 id="2-更改主题文件"><a href="#2-更改主题文件" class="headerlink" title="2. 更改主题文件"></a>2. 更改主题文件</h3><p>更改hexo/_config.yml中的theme: landscape为theme: yelee，再用hexo server就可以预览新主题。</p><h2 id="四、部署到github"><a href="#四、部署到github" class="headerlink" title="四、部署到github"></a>四、部署到github</h2><p>首先自己的github上要已经申请好了repo，repo的名称为username.github.io</p><h3 id="1-安装部署插件"><a href="#1-安装部署插件" class="headerlink" title="1. 安装部署插件"></a>1. 安装部署插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><h3 id="2-配置git"><a href="#2-配置git" class="headerlink" title="2. 配置git"></a>2. 配置git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: https://github.com/username/username.github.io</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><h3 id="3-生成和部署"><a href="#3-生成和部署" class="headerlink" title="3. 生成和部署"></a>3. 生成和部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g <span class="comment"># or hexo generate，用于生成静态网页</span></span><br><span class="line">hexo d <span class="comment"># or hexo deploy，用于部署到github</span></span><br></pre></td></tr></table></figure><h3 id="4-修改和添加内容之后"><a href="#4-修改和添加内容之后" class="headerlink" title="4. 修改和添加内容之后"></a>4. 修改和添加内容之后</h3><p>修改和添加内容之后，需要更新网页，进行再部署，首先要清空public/下的文件，然后重新生成部署：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>通过上述命令即可完成部署了，在username.github.io上就可以看到自己的网页啦，bravo！</p><h1 id="功能添加和改动"><a href="#功能添加和改动" class="headerlink" title="功能添加和改动"></a>功能添加和改动</h1><p>其他的简单功能在yelee的官方文档中已经有了详细的说明和指导，这里不再细述，主要讲一下自己添加的一些功能。</p><h2 id="一、评论"><a href="#一、评论" class="headerlink" title="一、评论"></a>一、评论</h2><p>评论功能是我放弃网易云笔记创建博客的初衷，希望能够促进技术交流。但是之前在使用jekyll的时候评论功能始终无法部署成功，因此选择了更简易的hexo，yelee默认支持的几个评论平台是Disqus、多说以及友言评论，Disqus需要翻墙，加载很慢，遂放弃，后两者好像已经停止服务了，因此默认支持的都不太行，所以我改成了使用<a href="https://github.com/gitalk/gitalk" target="_blank" rel="noopener">gitalk</a>，体验还可以。记录一下主要步骤和问题（参考链接为<a href="https://blog.wangriyu.wang/2018/03-valine.html" target="_blank" rel="noopener">yelee使用gitalk</a>）：</p><h3 id="1-注册github-OAuth-application"><a href="#1-注册github-OAuth-application" class="headerlink" title="1. 注册github OAuth application"></a>1. 注册github OAuth application</h3><p>因为gitalk是使用github的API实现评论的，评论内容实际上是保存在issue里面的，在链接<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">OAuth application申请</a>申请一个API，记下clientID和clientSecret，后面有用。</p><h3 id="2-修改主题配置文件"><a href="#2-修改主题配置文件" class="headerlink" title="2. 修改主题配置文件"></a>2. 修改主题配置文件</h3><p>在theme/yelee文件夹底下的_config.yml中添加下面的语句：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">gitalk:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  clientID:</span> <span class="string">'your client ID'</span></span><br><span class="line"><span class="attr">  clientSecret:</span> <span class="string">'your clientSecret'</span></span><br><span class="line"><span class="attr">  repo:</span> <span class="string">'username.github.io'</span> <span class="comment"># 仓库地址</span></span><br><span class="line"><span class="attr">  owner:</span> <span class="string">'username'</span> <span class="comment"># 拥有者</span></span><br><span class="line"><span class="attr">  admin:</span> <span class="string">['username']</span> <span class="comment"># admin 用户</span></span><br></pre></td></tr></table></figure><h3 id="3-创建页面描述文件"><a href="#3-创建页面描述文件" class="headerlink" title="3. 创建页面描述文件"></a>3. 创建页面描述文件</h3><p>在theme/yelee/layout/_partial/comments下创建gitalk.ejs，文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;section id=&apos;comments&apos; style=&apos;margin: 2em; padding: 2em; background: rgba(255, 255, 255, 0.5)&apos;&gt;</span><br><span class="line">  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt;</span><br><span class="line">  &lt;script src=&quot;https://unpkg.com/gitalk@latest/dist/gitalk.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt;</span><br><span class="line">  &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">    var gitalk = new Gitalk(&#123;</span><br><span class="line">      clientID: &apos;&lt;%= theme.gitalk.clientID %&gt;&apos;,</span><br><span class="line">      clientSecret: &apos;&lt;%= theme.gitalk.clientSecret %&gt;&apos;,</span><br><span class="line">      repo: &apos;&lt;%= theme.gitalk.repo %&gt;&apos;,</span><br><span class="line">      owner: &apos;&lt;%= theme.gitalk.owner %&gt;&apos;,</span><br><span class="line">      admin: [&apos;&lt;%= theme.gitalk.owner %&gt;&apos;],</span><br><span class="line">      id: window.location.pathname</span><br><span class="line">    &#125;)</span><br><span class="line">    gitalk.render(&apos;gitalk-container&apos;)</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&lt;/section&gt;</span><br></pre></td></tr></table></figure><h3 id="4-修改-partial-article-ejs"><a href="#4-修改-partial-article-ejs" class="headerlink" title="4. 修改_partial/article.ejs"></a>4. 修改_partial/article.ejs</h3><p>打开theme/yelee/layout/_partial/article.ejs，将gitalk组件加入文章的comment组件中去：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (!index &amp;&amp; post.comments)&#123; %&gt;</span><br><span class="line">    &lt;% if (theme.duoshuo.on) &#123; %&gt;</span><br><span class="line">      &lt;%- partial(&apos;comments/duoshuo&apos;, &#123;</span><br><span class="line">          key: post.path,</span><br><span class="line">          title: post.title,</span><br><span class="line">          url: config.url+url_for(post.path),</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">    &lt;% &#125; else if (theme.youyan.on) &#123; %&gt;</span><br><span class="line">        &lt;%- partial(&apos;comments/youyan&apos;) %&gt;</span><br><span class="line">    &lt;% &#125; else if (theme.disqus.on) &#123; %&gt;</span><br><span class="line">        &lt;%- partial(&apos;comments/disqus&apos;, &#123;</span><br><span class="line">            shortname: theme.disqus.shortname</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">    &lt;% &#125; else if (config.disqus_shortname) &#123; %&gt;</span><br><span class="line">        &lt;%- partial(&apos;comments/disqus&apos;, &#123;</span><br><span class="line">            shortname: config.disqus_shortname</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">+    &lt;% &#125; else if (theme.gitalk.on) &#123; %&gt;</span><br><span class="line">+        &lt;%- partial(&apos;comments/gitalk&apos;) %&gt;</span><br><span class="line">    &lt;% &#125; %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><h3 id="5-链接github-issue以及修改长度"><a href="#5-链接github-issue以及修改长度" class="headerlink" title="5. 链接github issue以及修改长度"></a>5. 链接github issue以及修改长度</h3><p>在这些文件修改完毕之后，就可以看到本地的网页上有gitalk显示了，但是本地部署的时候会提示没有关联到相关的github issue，这是因为本地部署的问题，这时候需要部署到github网页上，点击下方的登录github并关联，关联一下之后就可以了。</p><p>但是这样还会有一个小问题，那就是会在评论栏报一个错，导致无法评论，错误为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: Validation Failed</span><br></pre></td></tr></table></figure><p>这是因为issue的标签label有长度限制，而我们前面在1-4步时加入的label太长了，因此，将步骤3中的id: window.location.pathname改为id: ‘&lt;%= page.date %&gt;’，部署到网站之后，即可完成评论功能的添加。</p><p><img src="/2019/06/15/博客搭建日志/20190615-comments.png" alt="20190615-comments"></p><h2 id="二、其他功能"><a href="#二、其他功能" class="headerlink" title="二、其他功能"></a>二、其他功能</h2><p>其他功能还未添加，例如google analytics以及其他花里胡哨的功能，暂时不需要这样的功能，因此等有闲余时间想折腾了再来搞吧。</p>]]></content>
      
      
      <categories>
          
          <category> Techs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tech </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
