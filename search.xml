<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Burst Denoising with Kernel Prediction Networks</title>
      <link href="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/"/>
      <url>/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/1.png" alt="1"></p><p>这是18年CVPR利用KPN做burst去噪的文章，作者列表里赫然出现了Ren Ng大佬，有、东西～</p><a id="more"></a><h3 id="主要问题"><a href="#主要问题" class="headerlink" title="主要问题"></a>主要问题</h3><p>在去噪领域，多张图像的去噪往往可以利用求平均的方式，但是在使用手持相机的连拍图像里，涉及到相机motion或者场景motion的时候，直接求平均会带来很大的blur效果，去噪效果很差。文章提出使用空间可变的kernel来对齐各个图像并且做去噪，同时还提出了使用退火loss函数的方法。</p><p>拍照过程中的噪声分为两种，一个是shot noise，可以用Poisson过程来模拟，另一个是read noise，相机将raw data读取的时候的噪声，用一个高斯过程来模拟：<br>$$<br>x_{p} \sim \mathcal{N}\left(y_{p}, \sigma_{r}^{2}+\sigma_{s} y_{p}\right)<br>$$<br>其中$x_p$是带噪声的像素值，$y_p$是真实的intensity，对每张image，只要sensor gain是一致的，$\sigma _s$和$\sigma _r$就是确定的。真实图像连拍往往包括手抖动的motion和场景motion，这些都会影响去噪算法的处理。</p><p>burst denoising问题的定义：使用所有的图像对reference图像进行denoise。</p><h3 id="合成数据集"><a href="#合成数据集" class="headerlink" title="合成数据集"></a>合成数据集</h3><p>文章提出，为图像恢复任务收集真实数据集是个很有挑战性的事情，因为算法效果会受限于imaging system的能力，恢复出来的效果不会超过使用的imaging system。因此，文章使用openimage这个数据集提供的高清单帧图像，自行设计了模拟连拍的数据集生成方式（具体参见论文），并且在相机的参数曲线中随机挑选参数用于生成训练集，最后通过公式(1)来生成带噪声的连拍图像。</p><p><strong>可以看出这里的噪声图像和我们平常理解的加性、乘性噪声都不太一样</strong></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>OK，接下来就是我读这篇文章想要get的最重要的点：网络结构的设计。</p><p><img src="/2019/06/24/Burst-Denoising-with-Kernel-Prediction-Networks/Architecture.png" alt="Architecture"></p><p>文章用的是一种encoder-decoder的结构模式，输出是一个$K^2N$个通道的feature，feature大小与input的burst一样大，然后它们被reshape成每个pixel上都有N个$K\times K$大小的线性filter，输出需要经过如下的运算：<br>$$<br>\hat{Y}^{p}=\frac{1}{N} \sum_{i=1}^{N}\left\langle f_{i}^{p}, V^{p}\left(X_{i}\right)\right\rangle<br>$$<br>其中$\hat Y ^p$指的是p点上的输出值，$f_i^p$是指第i张image上p点对应的$K\times K$的kernel，$V^p(X_i)$是第i张image上p点的$K\times K$的邻域，所以生成的kernel实际上是per-pixel的卷积核。</p><h3 id="Loss上的改动"><a href="#Loss上的改动" class="headerlink" title="Loss上的改动"></a>Loss上的改动</h3><p>文章提出的loss是L2距离和梯度的L1距离（basic loss），但是实验观察到直接优化这个loss的话，网络会比较快地收敛到局部最优，具体体现的现象就是只有reference帧的filter是非0的。</p><p>作者提出主要的原因是multi-image的alignment和denoising是比单张的更难的，basic loss只会激励训练去考虑参考帧而不考虑其他帧的影响，所以才会导致网络快速收敛到局部最优。</p><p>鉴于此，文章提出退火loss：<br>$$<br>\mathcal{L}\left(X ; Y^{<em>}, t\right)=\ell\left(\frac{1}{N} \sum_{i=1}^{N} f_{i}\left(X_{i}\right), Y^{</em>}\right)+\beta \alpha^{t} \sum_{i=1}^{N} \ell\left(f_{i}\left(X_{i}\right), Y^{*}\right)<br>$$<br>其中$\beta$是一个超参，$0&lt;\alpha&lt;1$也是个超参，t则是代表iteration的次数，当$\beta \alpha^t \gg 1$时，第二项占据主导地位，说明网络在尝试把每一帧都独立地对齐到参考帧，作为使用basic loss的一种预训练，当t越来越大时，$\beta \alpha^t$趋近于0，从而前一个basic loss占据主导，网络学习着重新给出filter，使得对齐更好的帧可以对结果产生更好的影响，而对齐更差的帧的影响则被削弱。</p><p>个人感觉，本质上退火loss采用的思想是在优化的前期让网络去优化独立的align，使得各个帧能够具备初步的align能力，然后再使用basic loss去fusion和优化，这种思想和我们CSVT的EnhanceCNN想法是比较一致的，不同的就是我们是完全分开做的，他们则是更加巧妙地使用了退火loss把两步并做了一步。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>网络输入上，文章所提的网络除了输入burst，还将逐点估计的噪声水平（利用设定的相机参数$\sigma_s$和$\sigma_r$估计出来的）作为一个额外的channel输入进来了，这是一个比较有意思的想法：将成像过程中的参数或者与参数相关的东西作为额外的输入，往往有可能会有更好的效果；</li><li>网络结构上，通过生成per-pixel的卷积核，网络对align+去噪这一过程的模拟能力会更加优秀，因为得到的是一个滤波器组，会提供spatial-variant以及input-variant的kernel，这个input-variant不仅仅指输入的burst，还有输入的参数，这个很重要，会使得网络具备对不同条件下成像的burst的鲁棒性，文章的实验也证明了这一点；</li><li>退火loss的设计很巧妙，将我们正常思维中需要分步做的东西一步做到了，而且在local minimum的时候的分析也非常准确，是非常值得学习和借鉴的。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> denoising </tag>
            
            <tag> kernel prediction networks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Light field intrinsics with a deep encoder-decoder network</title>
      <link href="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/"/>
      <url>/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/1.png" alt="1"></p><p>这是承接上一篇文章读的，<a href="http://Joechann0831.github.io/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/">上一篇文章</a>是CVPRW19的利用3D的encoder-decoder加上GAN loss做SR的工作，它的结构其实比较奇怪，最后加上GAN loss的操作也有点莫名其妙，这篇CVPR18正会的文章就好多了。</p><a id="more"></a><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>文章的主要思想其实靠一张图就能看明白了，这也是我之前一直没有读这篇文章的原因，觉得看完图就懂了:</p><p><img src="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/Framework.png" alt="Framework"></p><p>EPI volume通过一个encoder编码到非常低维的空间，然后通过无监督方式训练AE得到表达、通过有监督的训练方式来得到disparity和intrinsic的两个分量。</p><p>一些小细节：</p><ol><li>输入是水平和垂直两个方向的EPI volume，共享网络参数，在disparity encoder之前concat在一起；</li><li>由于diffuse和specular相加是要等于LF的，所以在最后接近输出层时把二者的feature共享了；</li></ol><h3 id="Residual-block的设计"><a href="#Residual-block的设计" class="headerlink" title="Residual block的设计"></a>Residual block的设计</h3><p>文章还提出了它设计的encoder-decoder的res-block，与CVPRW19的是一样的结构，只不过deconv那里没有bicubic上采样这么神奇的操作而已：</p><p><img src="/2019/06/24/Light-field-intrinsics-with-a-deep-encoder-decoder-network/Res-block.png" alt="Res-block"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>总的来说，CVPRW19的文章结构上是沿袭了这篇18年的思想，只不过为什么用3D-2D这样的结构代替3D-3D的结构还是有点奇怪，如果单纯是为了结构上比较精简，倒也可以理解了。那么还有两个不太理解的地方就是，为什么要用GAN的loss，如果改用MSE loss比较PSNR和SSIM是否会更合理一些？</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> depth estimation </tag>
            
            <tag> intrinsic decomposition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Epipolar Volume Autoencoder with Adversarial Loss for Deep Light Field Super-Resolution</title>
      <link href="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/"/>
      <url>/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/1.png" alt="1"></p><p>这是19年CVPR NTIRE Workshop上的文章，也是做光场超分辨率的，而且还是第一次见到用生成对抗的loss来做光场的SR，应该挺有意思的。</p><a id="more"></a><p>看完之后，先讲讲优点吧。首先我觉得处理的问题是比较有创新性的，在给定非常少的view的情况下，能够同时做角度和空间的恢复，用到了3D的基于GAN的AE，见下图：</p><p><img src="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/2.png" alt="2"></p><p>输入是三个图中红色或者绿色框标记的view，经过一个3D的encoder得到一个$3\times 3\times 3$的latent表达，再经过9个2D的decoder，出来九个通道的角度SR结果，然后再经过两个(for 4x)或者一个(for 2x)空间超分辨率的网络，做到角度和空间上的超分辨率，最后用一个DiffWGAN的discriminator做判别器。</p><p>但是这里我有几个小疑问：</p><ol><li>为什么不用3D的encoder-decoder直接出来结果，而要用这种方式？节约解码端的计算资源？</li><li>最后为什么要用GAN？众所周知，GAN的discriminator是倾向于视觉质量的，目前光场的SR还没有到需要研究视觉质量的地步，即使这个判别器可能是为了判别输出的光场是否像真实的光场（不仅空间上很好，而且维持了很好的角度一致性），空间和角度一致性无法解耦也会导致空间恢复结果往perceptual靠拢。所以我觉得用GAN不是个特别好的主意，文章中说的是为了增强重建的光场的sharpness，引入了WGAN loss用来惩罚GT和estimation的角度和空间的导数之间的差，为什么不用MSE loss呢？用导数的MSE loss不也可以吗？</li></ol><p>文章的encoder-decoder的设计是参照他们之前在CVPR18里面发表的Light field intrinsics with a deep encoder-decoder network的文章，网络中设计了一些encoder res-block和decoder res-block，主要形式如下：</p><p><img src="/2019/06/19/An-Epipolar-Volume-Autoencoder-with-Adversarial-Loss-for-Deep-Light-Field-Super-Resolution/3.png" alt="3"></p><p>为啥要用Bicubic interpolation呢。。。这个的反传很复杂，而且好像还挺慢的，为啥不用bilinear？</p><p>最后的实验结果和14年PAMI、Bilinear、GB、SRGAN都比较了，但是这个比较看着挺混乱的，不晓得它到底在看重哪个指标，速度、accuracy、visual quality，总得有那么一两个吧？貌似挺混乱的。</p><p>不过这篇文章里面提到他们HCI实验室又做了一个新的数据集，大概就是之前说到的光场合成数据集，用于大型的深度学习任务的，目前有750个光场数据，之前他们在CVPR18提的那个数据集好像才将近200个。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Residual Networks for Light Field Image Super-Resolution</title>
      <link href="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/"/>
      <url>/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-1.png" alt="20190619-1-1"></p><p>这篇是CVPR19的做光场超分辨率的文章，是光场空间超分辨率这三四年以来第一次出现在顶会正会里面。其特点是恢复的accuracy很高，PSNR相比于SOTA高很多。</p><a id="more"></a><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>文章的网络结构没什么特别有新意的地方，跟EPINet是几乎一样的结构：</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-2.png" alt="20190619-1-2"></p><p>可以看出来，同样是multi-stream的结构，只不过加上了central view的分值提供residual的相加项而已，其实本质上是SR方法常用的技巧——把想要SR的view再以更shallow的方式加回去（数据处理不等式）。</p><h2 id="迁移到整个光场的SR"><a href="#迁移到整个光场的SR" class="headerlink" title="迁移到整个光场的SR"></a>迁移到整个光场的SR</h2><p>从上文可以看出，这种网络结构是和我们的CSVT一样，一次只能做一个view，文章提出使用一种灵活的策略来对整个光场做SR，其实主要就是针对不同角度位置来训练不同的网络，利用不同的view间信息：</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-3.png" alt="20190619-1-3"></p><p>以$7\times 7$的view为例，对central view，就用$7\times 7$的策略，对边上的view，根据其位置采用不同的策略，最后构成整个LF的恢复策略，如图中(e)所示。神奇的是，这种方式即使拿四个view，效果也很好（图中展示的是mona数据集的结果）。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="合成数据集"><a href="#合成数据集" class="headerlink" title="合成数据集"></a>合成数据集</h3><p>合成数据集的实验当然是HCI数据集了，其实验结果有一些可圈可点的地方，第一，除了它自己的结果，其他实验结果全是从LFNet的文章里面抄来的，第二，LFNet的文章里，bicubic的结果是不对的，但是我复现它的结果发现LFNet本身的结果确实就是那个数值。以下是合成数据集上的实验结果：</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-4.png" alt="20190619-1-4"></p><h3 id="真实数据集"><a href="#真实数据集" class="headerlink" title="真实数据集"></a>真实数据集</h3><p>文章的一个特点是实验做得比较完全，虽然网络结构确实没什么新意，从下表可以看出，实验结果确实很好，至于在现有的一些更好的SR方法里面是什么样的，也很难讲，例如TIP18的SAS和4D CNN，而且GB的效果在真实数据集里面这么好其实很让人惊讶，因为GB是基于matching的，真实数据集的暗角效应很影响matching，所以其实我是有点小疑问的，不过除了Lytro那个数据集，其他的要么我没有做实验，要么它的测试集划分我不晓得，所以也不好讲，毕竟GB有些时候效果也不错。</p><p><img src="/2019/06/18/Residual-Networks-for-Light-Field-Image-Super-Resolution/20190619-1-5.png" alt="20190619-1-5"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总而言之，这篇文章的网络结构，在利用光场多维数据上，想法是和EPINet一样的，那么idea上的问题也比较明显，没有用到所有的view，但是应该是把用到的view之间的关系用好了，所以效果很好。这说明光场SR的空间还是很大的，如果能够有一些方法把光场4D的信息很好地用上，相信效果会很好。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EPINet: A Fully Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images</title>
      <link href="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/"/>
      <url>/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/20190616-1.png" alt="20190616-1"></p><p>这是18年CVPR计算成像板块的一篇poster文章，因为网络结构和19年CVPR做光场SR的很像，所以在这里先读一下这篇，看看二者到底有什么区别。值得一提的是，这篇文章的团队是LFCNN的团队。</p><a id="more"></a><p>这篇文章的主要贡献有两个点：一个是网络结构，采用了一个multi-stream的网络结构，从不同的方向来获取EPI的信息，另一个就是它针对光场深度估计数据集太小做的一个数据增强，提出了一些light field specific的增强方法。</p><h2 id="Multi-stream网络结构"><a href="#Multi-stream网络结构" class="headerlink" title="Multi-stream网络结构"></a>Multi-stream网络结构</h2><p>网络结构图：</p><p><img src="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/20190618-2.png" alt="20190616-2"></p><p>最理想的利用EPI的方法，当然是从EPI中辨别出各个方向的极线，然后计算其斜率得到视差和深度，例如15年PAMI的结构张量方法，但是这种方法是一种特征检测+优化的方法，不使用于深度学习，因此，简化一下，从四个整数正切值的方向获取SAI的stack，分成不同的stream送入网络，将得到的特征concat在一起，再用8个conv block预测最后的disparity map。</p><p><strong>（感觉看完这几篇文章得再总结一下别人是怎么利用高维数据信息的，例如高维的卷积、一些独特设计的卷积结构等，我总觉得这其中有可以挖掘的地方。）</strong></p><h2 id="针对光场深度估计的数据增强方法"><a href="#针对光场深度估计的数据增强方法" class="headerlink" title="针对光场深度估计的数据增强方法"></a>针对光场深度估计的数据增强方法</h2><p>文章提出了很多数据增强方法：变化central view、空间旋转、resize所有view、翻转所有view、色彩调整、直方图拉伸等等。针对空间旋转，不是简单地旋转空间维度就完事了，见下图：</p><p><img src="/2019/06/16/EPINet-A-Fully-Convolutional-Neural-Network-Using-Epipolar-Geometry-for-Depth-from-Light-Field-Images/20190618-3.png" alt="20190616-3"></p><p>在旋转90度之后，可以看到横方向的SAI之间的视差已经从原本的横方向变成了纵方向，所以应该要把它送进纵方向的网络里面去，而且label也要对应地旋转。</p><p>resize的话，也是要对视差的值进行数值上的resize的。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>针对这篇文章，去年我看的时候并不觉得其网络结构有什么特别特殊的，但是今年CVPR有人拿它来做SR，效果还贼好，这就很尴尬了。。。一点想法：</p><ol><li>这种方法确实能够利用2D的卷积来比较不错地获取不同角度维的信息，但有一个缺陷，那就是它只能用到0、45、90、135这四个方向上的view，其他位置的view无法在multi-stream中被利用到，因此，如何充分地将所有的view之间的关系（这里所述的关系应当指的是角度上的邻接关系和不同角度的view之间的空间关系）用上仍然是一个值得思考的问题。但是不得不承认这种方式可以比较简单地利用到光场的角度维信息，尽管并不完全；</li><li>文章为了针对小的baseline，用的是$2\times2$的卷积核，其实很少见，但是文章没有比较其他的核大小，所以无法说明是否真的有效。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> light field </tag>
            
            <tag> depth estimation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建日志</title>
      <link href="/2019/06/15/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97/"/>
      <url>/2019/06/15/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<p>鼓捣了两天多的时间，终于把博客初步搭建好了，功能基本上都OK了，如果还想要别的（比如google analytics），就以后再加啦。</p><a id="more"></a><h1 id="关于使用jekyll还是hexo"><a href="#关于使用jekyll还是hexo" class="headerlink" title="关于使用jekyll还是hexo"></a>关于使用jekyll还是hexo</h1><p><a href="https://jekyllrb.com" target="_blank" rel="noopener">jekyll</a>和<a href="https://hexo.io" target="_blank" rel="noopener">hexo</a>是两个比较常用的静态网页框架，但是二者的体验是完全不一样的。这两天半的时间我主要是花在了jekyll上，使用的主题说明文档虽然很详细，但是涉及到的网页前端的底部知识太多了，依赖环境ruby也老是出问题，搞了老半天，搞了个半成品，地址在<a href="Joechann0831.github.io">失败作品</a>，只有简单的功能，因为复杂的功能都没加成功T_T</p><p>hexo就不一样了，它的开发文档很详细，而且操作很简单高效，加上用户群体多，社区体验很好，很多问题都能找到解决方案，因此使用hexo还是很好的。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><p>jekyll的搭建步骤就不详述了，太麻烦，而且我也没完全成功，这里主要记录一下使用hexo搭建博客的过程吧。</p><p>具体步骤是参考了<a href="https://blog.csdn.net/Gage__/article/details/80302471" target="_blank" rel="noopener">基于Ubuntu搭建Hexo个人博客</a>，这里加上了一些自己遇到的问题。</p><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><h3 id="1-安装Git"><a href="#1-安装Git" class="headerlink" title="1. 安装Git"></a>1. 安装Git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git-core -y</span><br></pre></td></tr></table></figure><h3 id="2-安装Node-js"><a href="#2-安装Node-js" class="headerlink" title="2. 安装Node.js"></a>2. 安装Node.js</h3><p>因为我在之前捣鼓jekyll的时候已经把这俩装好了，所以前面参考的博客里面的那三句话能不能用我也不晓得，就不记录了。</p><h2 id="二、-安装Hexo并初始化模板"><a href="#二、-安装Hexo并初始化模板" class="headerlink" title="二、 安装Hexo并初始化模板"></a>二、 安装Hexo并初始化模板</h2><h3 id="1-安装npm"><a href="#1-安装npm" class="headerlink" title="1. 安装npm"></a>1. 安装npm</h3><p>Hexo的相关依赖和包使用npm来安装的，因此要先安装npm，使用淘宝的源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><h3 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="2. 安装Hexo"></a>2. 安装Hexo</h3><p>创建一个自己的网页文件夹，例如我的是ZhenChengUSTC.github.io文件夹，cd到改文件夹下，执行下述命令安装Hexo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p><strong>注意使用上述两个npm命令时，最好加上sudo权限，否则会出现权限不足的问题</strong></p><h3 id="3-初始化Hexo文件夹"><a href="#3-初始化Hexo文件夹" class="headerlink" title="3. 初始化Hexo文件夹"></a>3. 初始化Hexo文件夹</h3><p>在该文件夹下初始化Hexo的模板：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>此时如果hexo安装所需的依赖没有安装，它会自动安装，只需要耐心等待。</p><h3 id="4-预览Hexo网页"><a href="#4-预览Hexo网页" class="headerlink" title="4. 预览Hexo网页"></a>4. 预览Hexo网页</h3><p>init结束之后，文件夹底下就已经有一个名为landscape的模板了，比较丑，但是我们可以预览一下确认Hexo安装是否正确：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure><p>hexo server默认端口是4000,如果端口被占用，可以更改端口或者解除占用，参考<a href="https://blog.csdn.net/yucicheung/article/details/79535282" target="_blank" rel="noopener">Ubuntu下用hexo搭建github博客</a>，这里不再赘述。</p><h2 id="三、更改主题"><a href="#三、更改主题" class="headerlink" title="三、更改主题"></a>三、更改主题</h2><p>初始的主体不太好看，我选择了yelee主题，这个主体比较符合我的审美而且有完整的<a href="http://moxfive.coding.me/yelee/" target="_blank" rel="noopener">中文说明文档</a>，很方便，具体步骤：</p><h3 id="1-git-clone主题文件"><a href="#1-git-clone主题文件" class="headerlink" title="1. git clone主题文件"></a>1. git clone主题文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:MOxFIVE/hexo-theme-yelee.git themes/yelee</span><br></pre></td></tr></table></figure><h3 id="2-更改主题文件"><a href="#2-更改主题文件" class="headerlink" title="2. 更改主题文件"></a>2. 更改主题文件</h3><p>更改hexo/_config.yml中的theme: landscape为theme: yelee，再用hexo server就可以预览新主题。</p><h2 id="四、部署到github"><a href="#四、部署到github" class="headerlink" title="四、部署到github"></a>四、部署到github</h2><p>首先自己的github上要已经申请好了repo，repo的名称为username.github.io</p><h3 id="1-安装部署插件"><a href="#1-安装部署插件" class="headerlink" title="1. 安装部署插件"></a>1. 安装部署插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><h3 id="2-配置git"><a href="#2-配置git" class="headerlink" title="2. 配置git"></a>2. 配置git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: https://github.com/username/username.github.io</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><h3 id="3-生成和部署"><a href="#3-生成和部署" class="headerlink" title="3. 生成和部署"></a>3. 生成和部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g <span class="comment"># or hexo generate，用于生成静态网页</span></span><br><span class="line">hexo d <span class="comment"># or hexo deploy，用于部署到github</span></span><br></pre></td></tr></table></figure><h3 id="4-修改和添加内容之后"><a href="#4-修改和添加内容之后" class="headerlink" title="4. 修改和添加内容之后"></a>4. 修改和添加内容之后</h3><p>修改和添加内容之后，需要更新网页，进行再部署，首先要清空public/下的文件，然后重新生成部署：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>通过上述命令即可完成部署了，在username.github.io上就可以看到自己的网页啦，bravo！</p><h1 id="功能添加和改动"><a href="#功能添加和改动" class="headerlink" title="功能添加和改动"></a>功能添加和改动</h1><p>其他的简单功能在yelee的官方文档中已经有了详细的说明和指导，这里不再细述，主要讲一下自己添加的一些功能。</p><h2 id="一、评论"><a href="#一、评论" class="headerlink" title="一、评论"></a>一、评论</h2><p>评论功能是我放弃网易云笔记创建博客的初衷，希望能够促进技术交流。但是之前在使用jekyll的时候评论功能始终无法部署成功，因此选择了更简易的hexo，yelee默认支持的几个评论平台是Disqus、多说以及友言评论，Disqus需要翻墙，加载很慢，遂放弃，后两者好像已经停止服务了，因此默认支持的都不太行，所以我改成了使用<a href="https://github.com/gitalk/gitalk" target="_blank" rel="noopener">gitalk</a>，体验还可以。记录一下主要步骤和问题（参考链接为<a href="https://blog.wangriyu.wang/2018/03-valine.html" target="_blank" rel="noopener">yelee使用gitalk</a>）：</p><h3 id="1-注册github-OAuth-application"><a href="#1-注册github-OAuth-application" class="headerlink" title="1. 注册github OAuth application"></a>1. 注册github OAuth application</h3><p>因为gitalk是使用github的API实现评论的，评论内容实际上是保存在issue里面的，在链接<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">OAuth application申请</a>申请一个API，记下clientID和clientSecret，后面有用。</p><h3 id="2-修改主题配置文件"><a href="#2-修改主题配置文件" class="headerlink" title="2. 修改主题配置文件"></a>2. 修改主题配置文件</h3><p>在theme/yelee文件夹底下的_config.yml中添加下面的语句：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">gitalk:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  clientID:</span> <span class="string">'your client ID'</span></span><br><span class="line"><span class="attr">  clientSecret:</span> <span class="string">'your clientSecret'</span></span><br><span class="line"><span class="attr">  repo:</span> <span class="string">'username.github.io'</span> <span class="comment"># 仓库地址</span></span><br><span class="line"><span class="attr">  owner:</span> <span class="string">'username'</span> <span class="comment"># 拥有者</span></span><br><span class="line"><span class="attr">  admin:</span> <span class="string">['username']</span> <span class="comment"># admin 用户</span></span><br></pre></td></tr></table></figure><h3 id="3-创建页面描述文件"><a href="#3-创建页面描述文件" class="headerlink" title="3. 创建页面描述文件"></a>3. 创建页面描述文件</h3><p>在theme/yelee/layout/_partial/comments下创建gitalk.ejs，文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;section id=&apos;comments&apos; style=&apos;margin: 2em; padding: 2em; background: rgba(255, 255, 255, 0.5)&apos;&gt;</span><br><span class="line">  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt;</span><br><span class="line">  &lt;script src=&quot;https://unpkg.com/gitalk@latest/dist/gitalk.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt;</span><br><span class="line">  &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">    var gitalk = new Gitalk(&#123;</span><br><span class="line">      clientID: &apos;&lt;%= theme.gitalk.clientID %&gt;&apos;,</span><br><span class="line">      clientSecret: &apos;&lt;%= theme.gitalk.clientSecret %&gt;&apos;,</span><br><span class="line">      repo: &apos;&lt;%= theme.gitalk.repo %&gt;&apos;,</span><br><span class="line">      owner: &apos;&lt;%= theme.gitalk.owner %&gt;&apos;,</span><br><span class="line">      admin: [&apos;&lt;%= theme.gitalk.owner %&gt;&apos;],</span><br><span class="line">      id: window.location.pathname</span><br><span class="line">    &#125;)</span><br><span class="line">    gitalk.render(&apos;gitalk-container&apos;)</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&lt;/section&gt;</span><br></pre></td></tr></table></figure><h3 id="4-修改-partial-article-ejs"><a href="#4-修改-partial-article-ejs" class="headerlink" title="4. 修改_partial/article.ejs"></a>4. 修改_partial/article.ejs</h3><p>打开theme/yelee/layout/_partial/article.ejs，将gitalk组件加入文章的comment组件中去：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (!index &amp;&amp; post.comments)&#123; %&gt;</span><br><span class="line">    &lt;% if (theme.duoshuo.on) &#123; %&gt;</span><br><span class="line">      &lt;%- partial(&apos;comments/duoshuo&apos;, &#123;</span><br><span class="line">          key: post.path,</span><br><span class="line">          title: post.title,</span><br><span class="line">          url: config.url+url_for(post.path),</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">    &lt;% &#125; else if (theme.youyan.on) &#123; %&gt;</span><br><span class="line">        &lt;%- partial(&apos;comments/youyan&apos;) %&gt;</span><br><span class="line">    &lt;% &#125; else if (theme.disqus.on) &#123; %&gt;</span><br><span class="line">        &lt;%- partial(&apos;comments/disqus&apos;, &#123;</span><br><span class="line">            shortname: theme.disqus.shortname</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">    &lt;% &#125; else if (config.disqus_shortname) &#123; %&gt;</span><br><span class="line">        &lt;%- partial(&apos;comments/disqus&apos;, &#123;</span><br><span class="line">            shortname: config.disqus_shortname</span><br><span class="line">          &#125;) %&gt;</span><br><span class="line">+    &lt;% &#125; else if (theme.gitalk.on) &#123; %&gt;</span><br><span class="line">+        &lt;%- partial(&apos;comments/gitalk&apos;) %&gt;</span><br><span class="line">    &lt;% &#125; %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><h3 id="5-链接github-issue以及修改长度"><a href="#5-链接github-issue以及修改长度" class="headerlink" title="5. 链接github issue以及修改长度"></a>5. 链接github issue以及修改长度</h3><p>在这些文件修改完毕之后，就可以看到本地的网页上有gitalk显示了，但是本地部署的时候会提示没有关联到相关的github issue，这是因为本地部署的问题，这时候需要部署到github网页上，点击下方的登录github并关联，关联一下之后就可以了。</p><p>但是这样还会有一个小问题，那就是会在评论栏报一个错，导致无法评论，错误为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: Validation Failed</span><br></pre></td></tr></table></figure><p>这是因为issue的标签label有长度限制，而我们前面在1-4步时加入的label太长了，因此，将步骤3中的id: window.location.pathname改为id: ‘&lt;%= page.date %&gt;’，部署到网站之后，即可完成评论功能的添加。</p><p><img src="/2019/06/15/博客搭建日志/20190615-comments.png" alt="20190615-comments"></p><h2 id="二、其他功能"><a href="#二、其他功能" class="headerlink" title="二、其他功能"></a>二、其他功能</h2><p>其他功能还未添加，例如google analytics以及其他花里胡哨的功能，暂时不需要这样的功能，因此等有闲余时间想折腾了再来搞吧。</p>]]></content>
      
      
      <categories>
          
          <category> Techs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tech </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
